<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <!--
    DOCUMENT PROVENANCE
    Title: Exponentials and Logarithms: Complete Lesson
    Version: 3
    Date: November 2025
    Source: Updated from v2.1 to comply with Technical Document Implementation v1.6
    
    Version History:
      v3 (2025-11-28): Updated to Technical Document Implementation v1.6
        - Added fullscreen CSS rules for graph containers
        - Added custom fullscreenButton to Plotly modebar
        - Added fullscreenchange event handler for Plotly resize
        - Wrapped all 18 plotly-graph divs with plotly-wrapper
        - Restored useful modebar buttons (zoom, pan, reset, download)
      v2.1 (2025-11): Fixed Plotly CDN URL, enabled fullscreen modebar, dual-panel graphs
      v2.0 (2025-11): Encoding fixes, MathJax v4 CHTML, Chart.js replaced with Plotly.js
      v1.0 (2025-10): Initial version
    
    Figure Inventory (18 Plotly graphs):
      Fig. 1.8-1: Logarithms in Different Bases (Plotly)
      Fig. 2.1-1: Exponential Function: y = 2^x (Plotly)
      Fig. 2.1-2: Exponential Decay: y = e^-x (Plotly)
      Fig. 2.2-1: Logarithmic Function: y = ln(x) (Plotly)
      Fig. 2.3-1: e^x and ln(x) are Inverse Functions (Plotly)
      Fig. 2.4-1: Linear-Linear Plot: Exponential Growth (Plotly)
      Fig. 2.4-2: Semi-Log Plot: Exponential Growth (Plotly)
      Fig. 2.4-3: Log-Log Plot: Power Law Relationship (Plotly)
      Fig. 4.2-1a: Bacterial Growth (Linear) (Plotly)
      Fig. 4.2-1b: Bacterial Growth (Log) (Plotly)
      Fig. 4.3-1: Carbon-14 Radioactive Decay (Plotly)
      Fig. 4.4-1: Newton's Law of Cooling (Plotly)
      Fig. 5.1-1a: Algorithm Complexity (Linear) (Plotly)
      Fig. 5.1-1b: Algorithm Complexity (Log) (Plotly)
      Fig. 6.3-1: Sigmoid Function (Plotly)
      Fig. 6.3-2: Softmax Function (Plotly)
      Fig. 6.4-1: Cross-Entropy Loss (Plotly)
      Fig. 7.2-1: Compound Interest Growth (Plotly)
    
    Guidelines Version: Technical Document Implementation v1.6
    -->
    <title>Exponentials and Logarithms: Complete Lesson</title>
    
    <!-- MathJax v4 for LaTeX equation rendering -->
    <script>
    window.MathJax = {
        tex: {
            inlineMath: [['$', '$'], ['\\(', '\\)']],
            displayMath: [['$$', '$$'], ['\\[', '\\]']],
            processEscapes: true
        },
        chtml: {
            displayOverflow: 'linebreak',
            displayAlign: 'left',
            scale: 1,
            minScale: 0.5,
            linebreaks: {
                inline: true,
                width: '100%',
                lineleading: 0.2
            }
        },
        options: {
            enableMenu: true,
            menuOptions: {
                settings: {
                    assistiveMml: true
                }
            }
        }
    };
    </script>
    <script src="https://cdn.jsdelivr.net/npm/mathjax@4/tex-chtml.js" id="MathJax-script" async></script>
    
    <!-- Plotly.js 2.35.2 for function plots -->
    <script src="https://cdn.plot.ly/plotly-2.35.2.min.js" charset="utf-8"></script>
    
    <style>
        body {
            font-family: 'Segoe UI', Tahoma, Geneva, Verdana, sans-serif;
            line-height: 1.6;
            color: #333;
            max-width: 900px;
            margin: 0 auto;
            padding: 20px;
            background-color: #f5f5f5;
        }

        .container {
            background-color: white;
            padding: 40px;
            border-radius: 10px;
            box-shadow: 0 2px 10px rgba(0,0,0,0.1);
        }

        h1 { color: #1e40af; border-bottom: 3px solid #3b82f6; padding-bottom: 10px; }
        h2 { color: #1e40af; margin-top: 30px; }
        h3 { color: #374151; }

        .highlight { background-color: #dbeafe; padding: 15px; border-left: 4px solid #3b82f6; margin: 20px 0; border-radius: 4px; }
        .example { background-color: #d1fae5; padding: 15px; border-left: 4px solid #10b981; margin: 20px 0; border-radius: 4px; }
        .warning { background-color: #fef3c7; padding: 15px; border-left: 4px solid #f59e0b; margin: 20px 0; border-radius: 4px; }
        .historical { background-color: #fce7f3; padding: 15px; border-left: 4px solid #ec4899; margin: 20px 0; border-radius: 4px; }
        .formula { text-align: center; font-size: 1.2em; margin: 20px 0; padding: 15px; background-color: #f3f4f6; border-radius: 4px; }

        code { background-color: #f3f4f6; padding: 2px 6px; border-radius: 3px; font-family: 'Courier New', monospace; }

        /* Figure container and numbering - for Plotly graphs */
        .graph-container { 
            margin: 30px auto; 
            padding: 20px; 
            background-color: #f9fafb; 
            border-radius: 8px;
            text-align: center;
            max-width: 800px;
        }

        .graph-title { 
            font-weight: bold; 
            color: #1e40af; 
            margin-bottom: 15px; 
            text-align: center; 
        }

        .graph-caption { 
            font-size: 0.9em; 
            color: #6b7280; 
            margin-top: 10px; 
            font-style: italic; 
            text-align: center; 
        }

        /* Plotly specific - Plotly manages its own internal styling */
        .plotly-graph {
            margin: 0 auto;
        }

        ul, ol { margin-left: 20px; }

        table {
            border-collapse: collapse;
            margin: 20px auto;
            background-color: white;
        }

        th, td {
            border: 1px solid #d1d5db;
            padding: 10px 15px;
            text-align: center;
        }

        th {
            background-color: #dbeafe;
            color: #1e40af;
            font-weight: bold;
        }

        .nav { 
            position: sticky; 
            top: 0; 
            background-color: white; 
            padding: 15px; 
            margin: -40px -40px 30px -40px; 
            border-bottom: 2px solid #e5e7eb; 
            z-index: 100; 
        }
        .nav a { color: #3b82f6; text-decoration: none; margin-right: 20px; font-weight: 500; }
        .nav a:hover { text-decoration: underline; }

        sub { vertical-align: sub; font-size: 0.8em; }
        sup { vertical-align: super; font-size: 0.8em; }

        .problem {
            background-color: #f3f4f6;
            padding: 15px;
            margin: 20px 0;
            border-radius: 4px;
            border-left: 4px solid #6b7280;
        }

        .solution {
            background-color: #f0fdf4;
            padding: 15px;
            margin: 10px 0 20px 20px;
            border-radius: 4px;
            border-left: 4px solid #10b981;
        }

        /* Mobile-specific adjustments */
        @media (max-width: 768px) {
            .plotly-graph {
                max-width: 100%;
            }
            
            code, pre {
                white-space: pre-wrap;
                word-break: break-word;
            }
        }

        /* Fullscreen styles for Plotly graph containers */
        .graph-container:fullscreen {
            background-color: white;
            display: flex;
            flex-direction: column;
            justify-content: center;
            align-items: center;
            padding: 20px;
            box-sizing: border-box;
        }

        .graph-container:fullscreen .graph-title {
            font-size: 1.5em;
            margin-bottom: 20px;
        }

        .graph-container:fullscreen .graph-caption {
            font-size: 1.1em;
            margin-top: 20px;
        }

        /* Override plotly-wrapper constraints in fullscreen */
        .graph-container:fullscreen .plotly-wrapper {
            max-width: 100% !important;
            width: 100% !important;
            margin: 0 !important;
        }

        /* Force Plotly div to expand - !important overrides inline styles */
        .graph-container:fullscreen .plotly-graph {
            width: 90vw !important;
            max-width: 90vw !important;
            height: 75vh !important;
            flex-grow: 1;
        }
    </style>
</head>
<body>
    <div class="container">
        <div class="nav">
            <a href="#intro">Introduction</a>
            <a href="#module1">Module 1</a>
            <a href="#module2">Module 2</a>
            <a href="#module3">Module 3</a>
            <a href="#module4">Module 4</a>
            <a href="#module5">Module 5</a>
            <a href="#module6">Module 6</a>
            <a href="#module7">Module 7</a>
        </div>

        <div id="intro">
            <h1>Exponentials and Logarithms: From Ancient Problems to Modern Computing</h1>
            <p style="font-size: 1.1em; color: #6b7280;">
                This course is intended to teach concepts regarding exponentials and logarithms for a motivated student. 
                It requires minimal background. Modules 1-3 are designed to provide the basics and foundation for understanding 
                and use of exponentials and logarithms, including history, applications, and real-world use across multiple fields. 
                Understanding exponentials and logarithms is essential for many technical fields and application areas. 
                Modules 4-7 are more advanced in nature. They provide a wider picture of how exp and log are foundational to 
                understanding many technical topics. It is not expected that a new student can master these topics from the 
                material presented, but they provide some more in-depth perspective and detail. If the intent is just to 
                understand the basics of exponentials and logarithms, Modules 4-7 can be skipped.
            </p>

            <div class="highlight">
                <h3>Course Structure (Total: ~10 hours)</h3>
                <ul>
                    <li><strong>Module 1:</strong> Foundations & Historical Context (2-2.5 hours)</li>
                    <li><strong>Module 2:</strong> Graphical Understanding (60-90 minutes)</li>
                    <li><strong>Module 3:</strong> Survey of Science Applications (90-120 minutes)</li>
                    <li><strong>Module 4:</strong> Computing Deep Dive (2-2.5 hours)</li>
                    <li><strong>Module 5:</strong> Modern CS Applications (30-45 minutes)</li>
                    <li><strong>Module 6:</strong> AI and Machine Learning (30-45 minutes)</li>
                    <li><strong>Module 7:</strong> Additional Applications and Next Steps (30-45 minutes)</li>
                </ul>
            </div>
        </div>

        <div id="module1">
            <h1>Module 1: Foundations & Historical Context</h1>

            <h2>1.1 The Problem That Started It All</h2>
            <p>
                Imagine you're an astronomer in 1600. You need to multiply: <strong>38,572 × 94,815</strong>
            </p>
            <p>
                Without calculators or computers, you'd spend 30+ minutes doing this by hand, with high risk of errors. 
                This was the daily reality for astronomers, navigators, and scientists before 1614. Astronomical calculations 
                required multiplying and dividing large numbers repeatedly—computing planetary positions, predicting eclipses, 
                navigating by the stars. The tedium was crushing, and errors were common.
            </p>

            <div class="historical">
                <strong>Before Logarithms (Pre-1614):</strong><br>
                Large multiplications were performed by hand using techniques like:
                <ul>
                    <li><strong>Long multiplication:</strong> Time-consuming and error-prone for 6-7 digit numbers</li>
                    <li><strong>Prosthaphaeresis:</strong> A clever trick using trigonometric identities to convert multiplication 
                    into addition. Developed around 1580 by Jost Bürgi and others, but required extensive trig tables and was 
                    limited in scope.</li>
                    <li><strong>Calculation by assistants:</strong> Wealthy scientists hired human "computers" to perform calculations, 
                    with multiple people checking each other's work</li>
                </ul>
                These methods were slow. Tycho Brahe's astronomical work in the 1580s-1600s required years of calculation work by teams of assistants.
            </div>

            <h2>1.2 Napier's Brilliant Insight</h2>
            
            <div class="historical">
                <strong>John Napier (1550-1617):</strong><br>
                A Scottish landowner, theologian, and mathematician. Napier spent approximately 20 years (from roughly 1594-1614) 
                developing logarithms. His motivation was explicitly to reduce the burden of calculation for astronomers and navigators.
                <ul>
                    <li><strong>1594-1614:</strong> Development period—Napier worked largely in isolation</li>
                    <li><strong>1614:</strong> Published <em>Mirifici Logarithmorum Canonis Descriptio</em> (Description of the 
                    Wonderful Canon of Logarithms)</li>
                    <li><strong>1615:</strong> Henry Briggs visited Napier in Scotland, amazed by the invention</li>
                    <li><strong>1617:</strong> Napier died; his work on logarithms published posthumously</li>
                    <li><strong>1624:</strong> Briggs published tables of base-10 logarithms, which became the standard</li>
                </ul>
            </div>

            <p>Napier noticed a pattern in powers:</p>
            <div class="formula">
                2<sup>3</sup> × 2<sup>5</sup> = 2<sup>(3+5)</sup> = 2<sup>8</sup>
            </div>
            <p style="text-align: center; font-size: 0.95em; color: #666;">
                (Read aloud: "Two to the third power times two to the fifth power equals two to the eighth power")
            </p>

            <div class="highlight">
                <strong>Key idea:</strong> If every number could be expressed as a power of some base, 
                multiplication would become addition of exponents!
            </div>

            <p>
                The word "logarithm" comes from Greek: <em>logos</em> (ratio) + <em>arithmos</em> (number), literally 
                meaning "ratio-number" or "number of ratios."
            </p>

            <div class="example">
                <strong>Napier's Conceptual Leap:</strong><br>
                Instead of thinking: "8 × 32 = ?"<br>
                Think: "2<sup>3</sup> × 2<sup>5</sup> = 2<sup>?</sup>"<br>
                Answer: 2<sup>(3+5)</sup> = 2<sup>8</sup> = 256
                <br><br>
                If you know that 8 = 2<sup>3</sup> and 32 = 2<sup>5</sup>, you've converted multiplication into addition!
            </div>

            <h2>1.3 The Revolutionary Impact</h2>
            
            <div class="historical">
                <strong>Immediate Adoption (1614-1630):</strong><br>
                Logarithms spread with remarkable speed:
                <ul>
                    <li><strong>1614-1620:</strong> Astronomers across Europe began using Napier's tables</li>
                    <li><strong>1620s:</strong> Edmund Gunter invented the logarithmic scale (precursor to slide rule)</li>
                    <li><strong>1622:</strong> William Oughtred invented the slide rule by putting two logarithmic scales together</li>
                    <li><strong>1624:</strong> Henry Briggs published base-10 logarithm tables (more practical than Napier's original base)</li>
                    <li><strong>1630s onward:</strong> Standard tool for scientists, navigators, engineers</li>
                </ul>
                
                <strong>Why so fast?</strong> Because logarithms reduced calculation time by a factor of 10-100. What took 
                an hour now took minutes. Pierre-Simon Laplace later said that logarithms "by shortening the labors, doubled 
                the life of the astronomer."
            </div>

            <h2>1.4 How Logarithms Work: The Definition</h2>
            <p>
                A logarithm answers the question: <em>"To what power must I raise the base to get this number?"</em>
            </p>
            
            <div class="formula">
                If b<sup>y</sup> = x, then log<sub>b</sub>(x) = y
            </div>
            <p style="text-align: center; font-size: 0.95em; color: #666;">
                Read aloud: "If b to the y equals x, then the log base b of x equals y"<br>
                Or more simply: "log base b of x asks: what power of b gives me x?"
            </p>

            <div class="example">
                <strong>Concrete Examples:</strong>
                <ul>
                    <li>log<sub>2</sub>(8) = 3 because 2<sup>3</sup> = 8 (ask: "2 to what power equals 8?")</li>
                    <li>log<sub>10</sub>(100) = 2 because 10<sup>2</sup> = 100</li>
                    <li>log<sub>10</sub>(1000) = 3 because 10<sup>3</sup> = 1000</li>
                    <li>log<sub>2</sub>(1024) = 10 because 2<sup>10</sup> = 1024</li>
                </ul>
            </div>

            <h2>1.5 Essential Properties of Logarithms</h2>
            <p>These properties are what made logarithms useful for computation. They allow you to convert multiplication 
            and division (slow) into addition and subtraction (fast).</p>
            
            <div class="example">
                <strong>Product Rule:</strong> log<sub>b</sub>(xy) = log<sub>b</sub>(x) + log<sub>b</sub>(y)<br>
                <em>Why?</em> Because b<sup>m</sup> · b<sup>n</sup> = b<sup>(m+n)</sup><br><br>
                
                <strong>Quotient Rule:</strong> log<sub>b</sub>(x/y) = log<sub>b</sub>(x) - log<sub>b</sub>(y)<br>
                <em>Why?</em> Because b<sup>m</sup> / b<sup>n</sup> = b<sup>(m-n)</sup><br><br>
                
                <strong>Power Rule:</strong> log<sub>b</sub>(x<sup>p</sup>) = p · log<sub>b</sub>(x)<br>
                <em>Why?</em> Because (b<sup>m</sup>)<sup>p</sup> = b<sup>(mp)</sup><br><br>
                
                <strong>Change of Base:</strong> log<sub>b</sub>(x) = log<sub>a</sub>(x) / log<sub>a</sub>(b)<br>
                <em>Why?</em> Let y = log<sub>b</sub>(x), so b<sup>y</sup> = x. Taking log<sub>a</sub> of both sides: 
                log<sub>a</sub>(b<sup>y</sup>) = log<sub>a</sub>(x). Using the power rule on the left: 
                y · log<sub>a</sub>(b) = log<sub>a</sub>(x). Therefore: y = log<sub>a</sub>(x) / log<sub>a</sub>(b).
                This lets you compute logs in any base using logs in a base you have tables for.
            </div>

            <h2>1.6 Logarithm Tables: The Killer App (1614-1970s)</h2>
            
            <div class="historical">
                <strong>The Era of Log Tables (1614-1975):</strong><br>
                For over 350 years, logarithm tables were essential tools for anyone doing numerical work.
                <ul>
                    <li><strong>Construction:</strong> Teams of human computers spent years calculating tables. Briggs computed 
                    his 14-decimal-place tables over decades. Kepler required 6 to 8 digit accuracy in the log tables he used 
                    for his astronomical calculations—calculating these log tables was itself a massive undertaking, though far 
                    less than trying to do brute force arithmetic to the needed precision without log tables. In the 1920s-1940s, 
                    WPA projects employed hundreds to create mathematical tables.</li>
                    <li><strong>Distribution:</strong> Log tables were printed in the back of math and science textbooks, 
                    issued to every student. Engineers and scientists kept personal copies.</li>
                    <li><strong>Ubiquity:</strong> Slide rules (mechanical logarithm computers) were standard issue for 
                    engineers from 1900-1975. NASA engineers used slide rules for Apollo calculations in the 1960s.</li>
                    <li><strong>End of Era:</strong> Electronic calculators (HP-35 in 1972, TI calculators mid-1970s) made 
                    tables obsolete almost overnight by 1980.</li>
                </ul>
            </div>

            <h2>1.7 Scientific Notation: The Key to Practical Log Tables</h2>
            
            <p>
                Before we can understand how log tables and slide rules work, we need to understand <strong>scientific notation</strong>—a way 
                of writing numbers that reveals their structure.
            </p>

            <h3>1.7.1 What is Scientific Notation?</h3>
            <p>
                Any positive number can be written as: <strong>a number between 1 and 10</strong> times <strong>a power of 10</strong>. 
                Negative numbers can also be written this way by simply including the negative sign.
            </p>

            <div class="formula">
                N = a × 10<sup>b</sup>
            </div>
            <p style="text-align: center; font-size: 0.95em; color: #666;">
                where 1 ≤ a < 10 (a is called the "mantissa" or "significand")<br>
                and b is an integer (b is called the "exponent" or "characteristic")
            </p>

            <div class="example">
                <strong>Scientific Notation Examples:</strong>
                <ul>
                    <li>237 = 2.37 × 10<sup>2</sup> (move decimal left 2 places)</li>
                    <li>48 = 4.8 × 10<sup>1</sup> (move decimal left 1 place)</li>
                    <li>5,280 = 5.280 × 10<sup>3</sup></li>
                    <li>94,815 = 9.4815 × 10<sup>4</sup></li>
                    <li>0.00372 = 3.72 × 10<sup>-3</sup> (move decimal right 3 places, so negative exponent)</li>
                    <li>-425 = -4.25 × 10<sup>2</sup> (negative sign stays with the number)</li>
                    <li>-0.0067 = -6.7 × 10<sup>-3</sup> (negative number with negative exponent)</li>
                    <li>6.02 × 10<sup>23</sup> (Avogadro's number—already in scientific notation)</li>
                </ul>
                
                <strong>Pattern:</strong> The exponent tells you how many places to move the decimal point. Positive exponent = 
                large number, negative exponent = small number.
            </div>

            <h3>1.7.2 Why Scientific Notation Makes Log Tables Practical</h3>
            
            <p>
                Here's the brilliant insight: Using the product rule for logarithms, we can split any logarithm into two parts:
            </p>

            <div class="formula">
                log<sub>10</sub>(a × 10<sup>b</sup>) = log<sub>10</sub>(a) + log<sub>10</sub>(10<sup>b</sup>) = log<sub>10</sub>(a) + b
            </div>

            <div class="highlight">
                <strong>Key Realization:</strong> 
                <ul>
                    <li>The integer part b (the exponent in scientific notation) is just... the exponent! No table needed.</li>
                    <li>The decimal part log<sub>10</sub>(a) comes from a table, but since 1 ≤ a < 10, you only need a table 
                    covering this small range!</li>
                </ul>
                
                This means instead of needing logarithm tables for all numbers from 1 to 1,000,000,000,000, you only need a table 
                for numbers from 1 to 10. The number of digits you use for the range 1 to 10 determines the precision of your logarithm.
            </div>

            <div class="example">
                <strong>How This Works:</strong><br><br>
                
                Find log<sub>10</sub>(237):<br>
                <ol>
                    <li><strong>Write in scientific notation:</strong> 237 = 2.37 × 10<sup>2</sup></li>
                    <li><strong>Apply log rules:</strong> log<sub>10</sub>(237) = log<sub>10</sub>(2.37 × 10<sup>2</sup>) 
                    = log<sub>10</sub>(2.37) + log<sub>10</sub>(10<sup>2</sup>)</li>
                    <li><strong>Simplify the power of 10:</strong> log<sub>10</sub>(10<sup>2</sup>) = 2 (because asking 
                    "10 to what power gives 10²?" obviously gives answer 2)</li>
                    <li><strong>Look up the mantissa:</strong> log<sub>10</sub>(2.37) ≈ 0.3747 (from the table)</li>
                    <li><strong>Add them:</strong> log<sub>10</sub>(237) = 0.3747 + 2 = 2.3747</li>
                </ol>

                If you need higher precision, you will need a table that calculates logs to more decimals. For example, 
                with a 5-digit log table, log(2.37) = 0.37475, more accurate than 0.3747.
                
                Notice the pattern in the answer:
                <ul>
                    <li>The integer part (2) tells you the order of magnitude—237 is between 10<sup>2</sup>=100 and 10<sup>3</sup>=1000</li>
                    <li>The decimal part (0.3747) comes from the table lookup for 2.37</li>
                </ul>
            </div>

            <div class="example">
                <strong>Another Example:</strong> Find log<sub>10</sub>(0.00482)<br><br>
                
                <ol>
                    <li><strong>Scientific notation:</strong> 0.00482 = 4.82 × 10<sup>-3</sup></li>
                    <li><strong>Apply log rules:</strong> log<sub>10</sub>(0.00482) = log<sub>10</sub>(4.82) + log<sub>10</sub>(10<sup>-3</sup>)</li>
                    <li><strong>Simplify:</strong> log<sub>10</sub>(10<sup>-3</sup>) = -3</li>
                    <li><strong>Look up:</strong> log<sub>10</sub>(4.82) ≈ 0.6830 (from table)</li>
                    <li><strong>Add:</strong> log<sub>10</sub>(0.00482) = 0.6830 + (-3) = -2.3170</li>
                </ol>
                
                Or written another way: -2.3170 = -3 + 0.6830, keeping the decimal part positive (this was the standard notation 
                in log tables, sometimes written as 3Ì".6830 where the bar means negative).
            </div>

            <div class="warning">
                <strong>Historical Note on Table Construction:</strong><br>
                Actual printed log tables typically listed values for numbers from 1.00 to 9.99 (in small increments like 0.01), 
                giving you the decimal part of the logarithm. For example:
                
                <table style="width: 50%; margin: 10px auto;">
                    <tr>
                        <th>N</th>
                        <th>log<sub>10</sub>(N)</th>
                    </tr>
                    <tr><td>2.37</td><td>0.3747</td></tr>
                    <tr><td>2.38</td><td>0.3766</td></tr>
                    <tr><td>2.39</td><td>0.3784</td></tr>
                    <tr><td>4.80</td><td>0.6812</td></tr>
                    <tr><td>4.81</td><td>0.6821</td></tr>
                    <tr><td>4.82</td><td>0.6830</td></tr>
                </table>
                
                For numbers with more digits (like 2.3746), you'd use interpolation between table entries—a simple calculation 
                to estimate values between the listed ones.
            </div>

            <div class="warning">
                <strong>Deep Dive: 4-Digit vs 5-Digit Log Tables</strong> <em>(Optional - can skip if desired)</em><br><br>
                
                Log tables use a lookup where the number of digits in table entries is based on the precision needed. 
                A 4-digit table would list numbers from 10 to 99, while 5-digit tables list from 100 to 999. This is best 
                illustrated by a specific example using the CRC Math Tables (first published in 1959), which shows logs for 
                numbers from 100 to 999 for finding four-digit logs, and numbers from 1000 to 9999 for finding five-digit logs. 
                The user adjusts the decimal point accordingly. It also has antilog tables.<br><br>
                
                <strong>Example: Compute 2.37 × 419.2 using logs</strong><br><br>
                
                <strong>Step 1:</strong> Convert to scientific notation<br>
                2.37 × 419.2 = 2.37 × 4.192 × 10<sup>2</sup><br><br>
                
                <strong>Step 2:</strong> Using 4-digit tables<br>
                log(2.37 × 419.2) = 2 + log(2.37) + log(4.192)<br>
                From 4-digit tables: log(2.37) ≈ 0.3747 and log(4.192) ≈ 0.6224<br>
                Sum: 2 + 0.3747 + 0.6224 = 2.9971<br>
                Answer: 10<sup>2</sup> × antilog(0.9971) ≈ 100 × 9.931 = 993.1<br>
                <em>(A modern calculator gives 993.504 to 5 decimal places)</em><br><br>
                
                <strong>Step 3:</strong> Using 5-digit tables for more precision<br>
                From 5-digit tables: log(2.37) = 0.37475 and log(4.192) = 0.62243<br>
                Sum: 2 + 0.37475 + 0.62243 = 2.99718<br>
                Answer: 10<sup>2</sup> × antilog(0.99718) ≈ 100 × 9.9337 = 993.37<br>
                <em>(Modern calculator: 993.504, so 5-digit table is much more accurate)</em><br><br>
                
                This example shows how table precision directly affects calculation accuracy. More digits in the table = 
                more accurate results, but also larger, more expensive books to print and distribute.
            </div>

            <h3>1.7.3 How to Use a Log Table</h3>
            <p>Now let's work through a concrete example of how someone in 1950 would multiply 237 × 48 using a log table. 
            You'll see how scientific notation and the product rule make this possible:</p>
            
            <div class="example">
                <strong>Problem:</strong> Calculate 237 × 48<br><br>
                
                <strong>Step 1 - Convert to scientific notation and find logarithms:</strong><br>
                
                For 237:
                <ul style="margin: 5px 0;">
                    <li>237 = 2.37 × 10<sup>2</sup></li>
                    <li>log<sub>10</sub>(237) = log<sub>10</sub>(2.37) + 2</li>
                    <li>From table: log<sub>10</sub>(2.37) ≈ 0.3747</li>
                    <li>Therefore: log<sub>10</sub>(237) = 0.3747 + 2 = 2.3747</li>
                </ul>
                
                For 48:
                <ul style="margin: 5px 0;">
                    <li>48 = 4.8 × 10<sup>1</sup></li>
                    <li>log<sub>10</sub>(48) = log<sub>10</sub>(4.8) + 1</li>
                    <li>From table: log<sub>10</sub>(4.8) ≈ 0.6812</li>
                    <li>Therefore: log<sub>10</sub>(48) = 0.6812 + 1 = 1.6812</li>
                </ul>
                
                <em>Notice: We only looked up two numbers in the table (2.37 and 4.8), both between 1 and 10!</em><br><br>
                
                <strong>Step 2 - Add the logarithms:</strong><br>
                log<sub>10</sub>(237 × 48) = log<sub>10</sub>(237) + log<sub>10</sub>(48)<br>
                = 2.3747 + 1.6812 = 4.0559<br>
                (Addition is much faster than multiplication!)<br><br>
                
                <strong>Step 3 - Find antilog (reverse the process):</strong><br>
                We need to find what number has logarithm 4.0559<br>
                <ul style="margin: 5px 0;">
                    <li>Split it: 4.0559 = 4 + 0.0559</li>
                    <li>The integer part (4) means the answer is something × 10<sup>4</sup></li>
                    <li>Look up 0.0559 in the antilog table: this corresponds to approximately 1.138</li>
                    <li>Therefore: answer = 1.138 × 10<sup>4</sup> = 11,380</li>
                </ul>
                
                <strong>Answer:</strong> 237 × 48 ≈ 11,380<br>
                (Actual answer: 11,376—the small error comes from rounding in the table lookups and interpolation)
            </div>

            <div class="warning">
                <strong>Why This Worked:</strong><br>
                By the product rule: log(237 × 48) = log(237) + log(48)<br>
                So if you add the logs and then "undo" the logarithm (take the antilog), you get the product!<br><br>
                
                <strong>The Genius of the Method:</strong><br>
                <ul>
                    <li>Scientific notation means you only need table entries for 1.00 to 9.99</li>
                    <li>The product rule converts multiplication into addition</li>
                    <li>A typical log table book was 50-100 pages but could handle any calculation</li>
                    <li>A 7-digit multiplication taking 2-3 minutes by hand → 30 seconds with log tables</li>
                    <li>Division was equally fast: log(a/b) = log(a) - log(b), so just subtract!</li>
                </ul>
                
                This combination of scientific notation + product rule + compact tables made logarithms the most important 
                computational tool for 350 years.
            </div>

            <div class="example">
                <strong>Division Example:</strong> Calculate 8,420 ÷ 175<br><br>
                
                <strong>Step 1 - Find logarithms using scientific notation:</strong><br>
                8,420 = 8.420 × 10<sup>3</sup><br>
                log<sub>10</sub>(8420) = log<sub>10</sub>(8.420) + 3 = 0.9253 + 3 = 3.9253<br><br>
                
                175 = 1.75 × 10<sup>2</sup><br>
                log<sub>10</sub>(175) = log<sub>10</sub>(1.75) + 2 = 0.2430 + 2 = 2.2430<br><br>
                
                <strong>Step 2 - Subtract the logarithms (quotient rule):</strong><br>
                log<sub>10</sub>(8420 ÷ 175) = 3.9253 - 2.2430 = 1.6823<br><br>
                
                <strong>Step 3 - Find antilog:</strong><br>
                1.6823 = 1 + 0.6823<br>
                Antilog of 0.6823 ≈ 4.81<br>
                Answer = 4.81 × 10<sup>1</sup> = 48.1<br><br>
                
                <strong>Check:</strong> 8420 ÷ 175 = 48.114... (log table gives 48.1—quite accurate!)
            </div>


            <h3>1.7.4 Slide Rules: Mechanical Logarithm Computers</h3>
            
            <p>
                While log tables required looking up values and doing arithmetic, the slide rule provided instant visual 
                calculation by physically representing logarithmic scales. A slide rule consists of two logarithmically-scaled 
                rulers that slide past each other, allowing you to add logarithms by physically adding distances—which corresponds 
                to multiplying the original numbers!
            </p>

            <div class="historical">
                <strong>The Slide Rule Era (1622-1975):</strong><br>
                <ul>
                    <li><strong>1622:</strong> William Oughtred invented the circular slide rule</li>
                    <li><strong>1850-1975:</strong> The straight slide rule became the standard tool for engineers, scientists, 
                    and students</li>
                    <li><strong>1960s:</strong> NASA engineers used slide rules for Apollo program calculations</li>
                    <li><strong>1970s:</strong> Electronic calculators rapidly replaced slide rules</li>
                </ul>
            </div>

            <div class="example">
                <strong>How a Slide Rule Works: Division Example</strong><br><br>
                
                Let's see how to calculate <strong>8 ÷ 4 = 2</strong> using a slide rule:<br><br>
                
                <div style="text-align: center; margin: 20px 0;">
                    <img src="data:image/png;base64,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" 
                         alt="Slide rule showing 8 divided by 4" 
                         style="max-width: 100%; height: auto; border: 1px solid #ccc; padding: 10px; background: white;">
                    <p style="font-size: 0.9em; color: #666; margin-top: 10px;">
                        Slide rule positioned to calculate 8 ÷ 4 = 2
                    </p>
                </div>

                <strong>Why This Works: The Logarithmic Principle</strong><br><br>
                
                The key insight is that the scales on a slide rule are <em>logarithmically spaced</em>, not linearly spaced:
                <ul>
                    <li>The distance from 1 to 2 on the scale represents log(2) ≈ 0.301</li>
                    <li>The distance from 1 to 4 represents log(4) ≈ 0.602</li>
                    <li>The distance from 1 to 8 represents log(8) ≈ 0.903</li>
                </ul>

                <strong>For Division (8 ÷ 4):</strong><br>
                The logarithm rule says: log(8 ÷ 4) = log(8) - log(4)<br><br>
                
                On the slide rule:
                <ol>
                    <li>Find 8 on the bottom (fixed) scale</li>
                    <li>Slide the top scale so that 4 on the slide aligns with 8 on the bottom</li>
                    <li>Read the answer at the "1" position of the slide scale: you'll find 2 on the bottom scale</li>
                </ol>
                
                <strong>Why it works mathematically:</strong><br>
                By aligning 4 (on top slide) with 8 (on bottom), you're physically setting up the equation:<br>
                "distance to 8" - "distance to 4" = "distance to answer"<br>
                Which is: log(8) - log(4) = log(8÷4) = log(2)<br><br>
                
                Reading at the "1" position tells you the answer is 2, because the physical distance from the slide's "1" 
                to the bottom scale represents the logarithm of the quotient, and you read the antilog directly!<br><br>
                
                <strong>For Multiplication:</strong> The process is similar but uses addition of logarithms instead of subtraction. 
                To multiply 2 × 4: align the slide's "1" with 2 on bottom, then read the answer (8) below the slide's 4 mark.
            </div>

            <div class="highlight">
                <strong>The Beauty of Slide Rules:</strong><br>
                A slide rule doesn't give you exact decimal digits—it gives you <em>visual estimation</em> to 3-4 significant 
                figures, which was sufficient for most engineering work. Engineers would determine the order of magnitude 
                separately (using scientific notation in their heads), then read the mantissa from the slide rule. This made 
                them incredibly fast for practical calculations, though less precise than log tables for critical work.
            </div>
            <h2>1.8 Common Bases and Why They're Used</h2>
            
            <h3>1.8.1 Base 10 (Common Logarithm)</h3>
            <p>
                Written as log<sub>10</sub>(x) or often just log(x) when the base is clear from context.
            </p>
            
            <div class="historical">
                <strong>Why Base 10?</strong><br>
                Henry Briggs (1561-1630), professor at Gresham College London, visited Napier in 1615 and proposed base 10 
                instead of Napier's original base (which was close to 1/e). Reason: Humans count in base 10, and our number 
                system is base 10. This made tables much more intuitive.
                <ul>
                    <li>log<sub>10</sub>(10) = 1, log<sub>10</sub>(100) = 2, log<sub>10</sub>(1000) = 3</li>
                    <li>The integer part tells you the number of digits (roughly)</li>
                    <li>Aligned perfectly with scientific notation: 6.02 × 10<sup>23</sup></li>
                </ul>
                Briggs' base-10 tables became the standard and remained so for 350 years.
            </div>
            
            <p><strong>Where it's used today:</strong> pH scale, decibels, Richter scale (earthquake magnitude), any application 
            measuring things across many orders of magnitude in powers of 10.</p>

            <h3>1.8.2 Base e (Natural Logarithm)</h3>
            <p>
                Written as ln(x), which stands for "logarithmus naturalis" (Latin for natural logarithm).<br>
                The base is e ≈ 2.71828..., an irrational number.
            </p>
            
            <div class="historical">
                <strong>The Mysterious Number e:</strong><br>
                <ul>
                    <li><strong>1618:</strong> John Napier's table appendix contained calculations of values that implied e, 
                    though he didn't identify it as a special number</li>
                    <li><strong>1683:</strong> Jacob Bernoulli studied compound interest: $(1 + 1/n)^n$ as n increases. 
                    Found it approached approximately 2.718...</li>
                    <li><strong>1690s:</strong> Gottfried Leibniz used the letter "b" for this constant in work on calculus</li>
                    <li><strong>1727-1728:</strong> Leonhard Euler studied the constant extensively, proved it was irrational, 
                    and began using the letter "e" (possibly for "exponential")</li>
                    <li><strong>1748:</strong> Euler published <em>Introductio in analysin infinitorum</em>, establishing e 
                    as fundamental to mathematics</li>
                </ul>
                
                <strong>Why "Natural"?</strong> Called "natural" because it emerges automatically from calculus. The function 
                f(x) = e<sup>x</sup> has the unique property that its rate of change (derivative) equals itself. This makes it 
                the natural choice for modeling any process where rate of change is proportional to current amount—which describes 
                most growth and decay in nature.
            </div>

            <p><strong>Where it's used today:</strong> Continuous growth/decay processes, probability theory, calculus, physics, 
            engineering, any differential equations, compounding interest calculated continuously.</p>

            <h3>1.8.3 Base 2 (Binary Logarithm)</h3>
            <p>
                Written as log<sub>2</sub>(x) or sometimes lg(x) in computer science contexts.
            </p>
            
            <div class="historical">
                <strong>The Binary System and Computing:</strong><br>
                <ul>
                    <li><strong>Ancient:</strong> Binary concepts existed (yin/yang, I Ching's binary divination)</li>
                    <li><strong>1703:</strong> Gottfried Leibniz described binary arithmetic, though primarily as philosophical curiosity</li>
                    <li><strong>1854:</strong> George Boole developed Boolean algebra (binary logic)</li>
                    <li><strong>1937:</strong> Claude Shannon's master's thesis showed Boolean algebra could represent 
                    electrical switching circuits—the foundation of digital computers</li>
                    <li><strong>1945-1950s:</strong> First electronic digital computers (ENIAC, EDVAC) implemented binary arithmetic</li>
                    <li><strong>1950s-present:</strong> All modern computers use binary because electronic circuits have two 
                    stable states: on (1) and off (0)</li>
                </ul>
                
                <strong>Why Base 2 for Computers?</strong><br>
                Electronic circuits naturally have two states—voltage high or low, current on or off. While you could build 
                10-state circuits (base 10), reliably distinguishing 10 different voltage levels is much harder than 
                distinguishing 2. Binary is simple, reliable, and fast.
            </div>

            <p><strong>Where it's used today:</strong> Computer science and information theory. log<sub>2</sub>(n) answers: 
            "How many times do I halve n to reach 1?" or equivalently "How many bits do I need to represent numbers from 0 to n?"</p>

            <div class="graph-container">
                <div class="graph-title">Fig. 1.8-1: Logarithms in Different Bases</div>
                <div class="plotly-wrapper" style="position: relative; max-width: 700px; margin: 0 auto;">
                    <div id="logBasesGraph" class="plotly-graph" style="width:100%; height:400px;"></div>
                </div>
                <div class="graph-caption">
                    All logarithms pass through (1, 0). Smaller bases grow faster: log₂ > ln > log₁₀
                </div>
            </div>

            <div class="highlight">
                <strong>Practice Problems:</strong><br>
                1. Calculate by hand: log<sub>2</sub>(16) = ?<br>
                2. Calculate by hand: log<sub>10</sub>(10,000) = ?<br>
                3. If log<sub>10</sub>(5) ≈ 0.699, estimate log<sub>10</sub>(50) without a calculator.<br>
                4. Why does every logarithm, regardless of base, pass through the point (1, 0)?<br><br>
                
                <strong>Answers:</strong><br>
                1. log<sub>2</sub>(16) = 4 because 2<sup>4</sup> = 16<br>
                2. log<sub>10</sub>(10,000) = 4 because 10<sup>4</sup> = 10,000<br>
                3. log<sub>10</sub>(50) = log<sub>10</sub>(5 × 10) = log<sub>10</sub>(5) + log<sub>10</sub>(10) ≈ 0.699 + 1 = 1.699<br>
                4. Because b<sup>0</sup> = 1 for any base b, so log<sub>b</sub>(1) = 0
            </div>
        </div>

        <div id="module2">
            <h1>Module 2: Graphical Understanding</h1>

            <h2>2.1 The Exponential Function</h2>
            <p>
                An exponential function has the form f(x) = b<sup>x</sup>, where b is called the base (b > 0, b ≠ 1).
                The variable appears in the exponent—that's what makes it "exponential."
            </p>

            <div class="example">
                <strong>Common forms you'll see:</strong>
                <ul>
                    <li>f(x) = 2<sup>x</sup> (base 2—used in computer science)</li>
                    <li>f(x) = e<sup>x</sup> (base e—used in calculus, natural sciences)</li>
                    <li>f(x) = 10<sup>x</sup> (base 10—used when working with powers of 10)</li>
                </ul>
            </div>

            <h3>2.1.1 Key Properties</h3>
                <li><strong>Passes through (0, 1):</strong> because b<sup>0</sup> = 1 for any base b</li>
                <li><strong>Growth behavior:</strong> For b > 1, the function grows without bound as x increases</li>
                <li><strong>Decay behavior:</strong> As x decreases (goes negative), b<sup>x</sup> approaches 0 but never reaches it</li>
            </ul>

            <div class="graph-container">
                <div class="graph-title">Fig. 2.1-1: Exponential Function: y = 2<sup>x</sup></div>
                <div class="plotly-wrapper" style="position: relative; max-width: 700px; margin: 0 auto;">
                    <div id="exp2xGraph" class="plotly-graph" style="width:100%; height:400px;"></div>
                </div>
                <div class="graph-caption">
                    The exponential function passes through (0,1) and approaches but never touches y = 0
                </div>
            </div>

            <h3>2.1.2 Exponential Decay: The Function e<sup>-x</sup></h3>
            <p>
                While e<sup>x</sup> represents exponential growth, the function e<sup>-x</sup> (where x is positive) represents 
                exponential decay. This is one of the most important functions in science and engineering.
            </p>

            <div class="example">
                <strong>Understanding e<sup>-x</sup>:</strong><br>
                When x is positive, e<sup>-x</sup> = 1/e<sup>x</sup>, which decreases as x increases.
                <ul>
                    <li>e<sup>0</sup> = 1 (starts at 1)</li>
                    <li>e<sup>-1</sup> ≈ 0.368</li>
                    <li>e<sup>-2</sup> ≈ 0.135</li>
                    <li>e<sup>-5</sup> ≈ 0.0067</li>
                    <li>e<sup>-10</sup> ≈ 0.000045</li>
                </ul>
                As x increases, e<sup>-x</sup> rapidly approaches 0 but never reaches it.
            </div>

            <div class="graph-container">
                <div class="graph-title">Fig. 2.1-2: Exponential Decay: y = e<sup>−x</sup></div>
                <div class="plotly-wrapper" style="position: relative; max-width: 700px; margin: 0 auto;">
                    <div id="expNegXGraph" class="plotly-graph" style="width:100%; height:400px;"></div>
                </div>
                <div class="graph-caption">
                    The exponential decay function e<sup>-x</sup> passes through (0,1) and decreases rapidly, 
                    approaching but never reaching y = 0. This function models radioactive decay, cooling, 
                    and many other natural processes.
                </div>
            </div>

            <h2>2.2 The Logarithmic Function</h2>
            <p>
                A logarithmic function has the form f(x) = log<sub>b</sub>(x). It's defined only for x > 0 
                (you cannot take the logarithm of zero or negative numbers—we'll explain why below).
            </p>

            <h3>2.2.1 Key Properties</h3>
                <li><strong>Passes through (1, 0):</strong> because log<sub>b</sub>(1) = 0 for any base b</li>
                <li><strong>Vertical asymptote at x = 0:</strong> As x approaches 0 from the right, log<sub>b</sub>(x) → -∞</li>
                <li><strong>Growth behavior:</strong> Increases slowly—much slower than linear growth</li>
            </ul>

            <div class="graph-container">
                <div class="graph-title">Fig. 2.2-1: Logarithmic Function: y = ln(x)</div>
                <div class="plotly-wrapper" style="position: relative; max-width: 700px; margin: 0 auto;">
                    <div id="lnGraph" class="plotly-graph" style="width:100%; height:400px;"></div>
                </div>
                <div class="graph-caption">
                    The logarithm passes through (1,0) and has a vertical asymptote at x = 0
                </div>
            </div>

            <div class="warning">
                <strong>Why is ln(x) undefined for x ≤ 0?</strong><br>
                Remember that ln(x) asks: "To what power must I raise e to get x?" Since e<sup>y</sup> is always 
                positive for any real y, there's no power that gives zero or negative results. Try it: e<sup>10</sup> = 22026... 
                (huge but positive), e<sup>0</sup> = 1 (positive), e<sup>-100</sup> = 0.0000...00037 (tiny but positive). 
                No exponent makes e<sup>y</sup> equal 0 or negative.
            </div>

            <h2>2.3 Inverse Functions: The Mirror Relationship</h2>
            <p>
                Exponential and logarithmic functions are inverses of each other. This means they "undo" each other:
            </p>
            <ul>
                <li>If f(x) = e<sup>x</sup>, then f<sup>-1</sup>(x) = ln(x)</li>
                <li>e<sup>ln(x)</sup> = x for all x > 0</li>
                <li>ln(e<sup>x</sup>) = x for all real x</li>
            </ul>

            <p>
                Graphically, inverse functions are mirror images across the line y = x. Every point (a, b) on one function 
                corresponds to the point (b, a) on its inverse.
            </p>
            
            <div class="graph-container">
                <div class="graph-title">Fig. 2.3-1: e<sup>x</sup> and ln(x) are Inverse Functions</div>
                <div class="plotly-wrapper" style="position: relative; max-width: 700px; margin: 0 auto;">
                    <div id="inverseGraph" class="plotly-graph" style="width:100%; height:400px;"></div>
                </div>
                <div class="graph-caption">
                    The exponential e<sup>x</sup> (blue) and logarithm ln(x) (green) are mirror images across y = x (gray dashed line). 
                    Each point (a,b) on one curve corresponds to (b,a) on the other.
                </div>
            </div>

            <div class="example">
                <strong>Understanding the Reflection:</strong><br>
                The point (0, 1) is on the exponential curve y = e<sup>x</sup> because e<sup>0</sup> = 1.<br>
                The reflected point (1, 0) is on the logarithmic curve y = ln(x) because ln(1) = 0.<br><br>
                
                Similarly, (1, e) ≈ (1, 2.72) is on y = e<sup>x</sup>, and (e, 1) ≈ (2.72, 1) is on y = ln(x).<br>
                The functions perfectly mirror each other across the 45-degree line y = x.
            </div>

            <p>
                This inverse relationship is why logarithms solve exponential equations: if e<sup>x</sup> = 7, 
                then taking ln of both sides gives x = ln(7) ≈ 1.95.
            </p>

            <h2>2.4 Linear vs. Logarithmic Axes</h2>
            
            <p>
                When graphing data, we can choose whether to use linear or logarithmic scales for our axes. This choice 
                dramatically affects how we visualize and interpret the data. Understanding when to use each type is crucial 
                for working with exponential and logarithmic relationships.
            </p>

            <h3>2.4.1 Linear-Linear Plot</h3>
            <p>
                In a linear-linear plot, both axes use regular arithmetic spacing. Each unit of distance represents the same 
                numerical increment. This is what we typically use and what you've seen in all the graphs so far.
            </p>

            <div class="graph-container">
                <div class="graph-title">Fig. 2.4-1: Linear-Linear Plot: Exponential Growth</div>
                <div class="plotly-wrapper" style="position: relative; max-width: 700px; margin: 0 auto;">
                    <div id="linearLinearGraph" class="plotly-graph" style="width:100%; height:400px;"></div>
                </div>
                <div class="graph-caption">
                    With linear axes, exponential growth appears as a J-curve. Early values are hard to see because 
                    later values dominate the scale. This makes it difficult to see detail in the early stages.
                </div>
            </div>

            <h3>2.4.2 Semi-Log Plot (Linear x-axis, Logarithmic y-axis)</h3>
            <p>
                In a semi-log plot, the y-axis uses logarithmic spacing where each step represents multiplication by a constant factor 
                (typically 10). This is incredibly useful for data that spans many orders of magnitude.
            </p>

            <div class="highlight">
                <strong>Key Property of Semi-Log Plots:</strong><br>
                On a semi-log plot, exponential growth appears as a straight line! This is because if y = e<sup>kx</sup>, 
                then log(y) = kx, which is linear.
            </div>

            <div class="graph-container">
                <div class="graph-title">Fig. 2.4-2: Semi-Log Plot: Exponential Growth</div>
                <div class="plotly-wrapper" style="position: relative; max-width: 700px; margin: 0 auto;">
                    <div id="semiLogGraph" class="plotly-graph" style="width:100%; height:400px;"></div>
                </div>
                <div class="graph-caption">
                    The same exponential data appears as a straight line on a semi-log plot. This makes it easy to:
                    (1) identify exponential growth, (2) see all values clearly, (3) measure growth rates.
                </div>
            </div>

            <div class="example">
                <strong>When to Use Semi-Log Plots:</strong>
                <ul>
                    <li><strong>Bacterial growth:</strong> Population doubles regularly—straight line on semi-log shows consistent growth rate</li>
                    <li><strong>Radioactive decay:</strong> Activity decreases exponentially—straight line shows constant decay rate</li>
                    <li><strong>Computer performance:</strong> Moore's Law (transistors doubling) appears linear on semi-log plot</li>
                    <li><strong>Earthquake magnitudes:</strong> Richter scale is logarithmic because earthquake energies span huge ranges</li>
                    <li><strong>Sound intensity:</strong> Decibels use log scale because our ears respond logarithmically</li>
                </ul>
            </div>

            <h3>2.4.3 Log-Log Plot (Both Axes Logarithmic)</h3>
            <p>
                When both axes use logarithmic scales, we can visualize power-law relationships. If y = x<sup>n</sup>, 
                then log(y) = n·log(x), which is linear on a log-log plot with slope n.
            </p>

            <div class="graph-container">
                <div class="graph-title">Fig. 2.4-3: Log-Log Plot: Power Law Relationship</div>
                <div class="plotly-wrapper" style="position: relative; max-width: 700px; margin: 0 auto;">
                    <div id="logLogGraph" class="plotly-graph" style="width:100%; height:400px;"></div>
                </div>
                <div class="graph-caption">
                    On a log-log plot, power laws y = x<sup>n</sup> appear as straight lines. The slope of the line 
                    equals the exponent n. Shown: y = x² (slope 2), y = x (slope 1), and y = √x (slope 0.5).
                </div>
            </div>

            <div class="example">
                <strong>When to Use Log-Log Plots:</strong>
                <ul>
                    <li><strong>Physics:</strong> Many physical laws are power laws (gravity ∝ 1/r², intensity ∝ 1/r²)</li>
                    <li><strong>Biology:</strong> Metabolic rate scales as mass<sup>3/4</sup> across species</li>
                    <li><strong>Economics:</strong> Zipf's law (frequency vs. rank follows power law)</li>
                    <li><strong>Network science:</strong> Many networks show power-law degree distributions</li>
                </ul>
            </div>

            <div class="warning">
                <strong>Important Note:</strong><br>
                Logarithmic axes can only show positive values (since log of 0 or negative numbers is undefined). 
                If your data includes zero or negative values, you cannot use a logarithmic scale for that axis.
            </div>

            <h2>2.5 Real-World Example: Bacterial Growth</h2>
            
            <div class="historical">
                <strong>History of Understanding Bacterial Growth:</strong>
                <ul>
                    <li><strong>1676:</strong> Antonie van Leeuwenhoek first observed bacteria using microscopes</li>
                    <li><strong>1881:</strong> Robert Koch developed methods for pure bacterial cultures</li>
                    <li><strong>1900s-1910s:</strong> Bacteriologists observed that cultures grew exponentially initially, 
                    then leveled off</li>
                    <li><strong>1920:</strong> Raymond Pearl and Lowell Reed formalized the logistic growth model, 
                    explaining why exponential growth couldn't continue indefinitely</li>
                    <li><strong>1925-1930:</strong> Pierre François Verhulst's earlier work (1838) on population dynamics 
                    was rediscovered and applied to bacterial growth</li>
                </ul>
                
                <strong>The Discovery:</strong> Scientists noticed bacteria in petri dishes grew explosively at first, then 
                growth slowed and stopped. This led to understanding carrying capacity—the maximum population sustainable in 
                a given environment.
            </div>

            <h3>2.5.1 The Mathematical Models</h3>
            
            <div class="example">
                <strong>Exponential Growth Model:</strong><br>
                N(t) = N<sub>0</sub> · e<sup>rt</sup><br><br>
                
                <strong>Parameters:</strong>
                <ul>
                    <li>N(t) = population at time t</li>
                    <li>N<sub>0</sub> = initial population (at t = 0)</li>
                    <li>r = growth rate (determines how fast population increases)</li>
                    <li>t = time</li>
                </ul>
                
                <strong>Assumption:</strong> Resources are unlimited, so population can grow without bound. 
                This model works well for short time periods when resources are abundant.
            </div>

            <div class="example">
                <strong>Logistic Growth Model:</strong><br>
                N(t) = K / (1 + A · e<sup>-rt</sup>)<br><br>
                
                <strong>Parameters:</strong>
                <ul>
                    <li>N(t) = population at time t</li>
                    <li>K = carrying capacity (maximum sustainable population)</li>
                    <li>A = (K - N<sub>0</sub>) / N<sub>0</sub> (a constant determined by initial conditions)</li>
                    <li>r = growth rate (same as in exponential model)</li>
                    <li>t = time</li>
                </ul>
                
                <strong>Assumption:</strong> Growth slows as population approaches carrying capacity K. 
                As N approaches K, the growth rate decreases to zero. This model better matches reality for longer time periods.
            </div>
            
            <div class="graph-container">
                <div class="graph-title">Fig. 2.5-1a: Bacterial Growth — Early Phase (Linear Scale)</div>
                <div class="plotly-wrapper" style="position: relative; max-width: 700px; margin: 0 auto;">
                    <div id="bacterialGraphLinear" class="plotly-graph" style="width:100%; height:350px;"></div>
                </div>
                <div class="graph-caption">
                    Early growth phase (t = 0 to 8 hours) on linear scale. Parameters: N₀ = 10, r = 0.5/hour, K = 1000.
                    Both models appear similar initially, but exponential (blue) begins to diverge from logistic (green).
                </div>
            </div>
            
            <div class="graph-container">
                <div class="graph-title">Fig. 2.5-1b: Bacterial Growth — Full Time Range (Semi-Log Scale)</div>
                <div class="plotly-wrapper" style="position: relative; max-width: 700px; margin: 0 auto;">
                    <div id="bacterialGraphLog" class="plotly-graph" style="width:100%; height:350px;"></div>
                </div>
                <div class="graph-caption">
                    Full 24-hour period on semi-log scale reveals the dramatic difference: exponential reaches ~160,000 
                    while logistic asymptotes to carrying capacity K = 1000. The log scale makes both curves visible.
                </div>
            </div>

            <div class="warning">
                <strong>Why Does the Exponential Model Break Down?</strong><br>
                The exponential model N(t) = N<sub>0</sub> · e<sup>rt</sup> assumes:
                <ul>
                    <li>Unlimited nutrients</li>
                    <li>Unlimited space</li>
                    <li>No waste accumulation</li>
                    <li>No competition</li>
                </ul>
                
                Reality introduces constraints:
                <ul>
                    <li>Food gets depleted</li>
                    <li>The petri dish fills up</li>
                    <li>Toxic waste products accumulate</li>
                    <li>Interior cells can't access nutrients</li>
                </ul>
                
                During hours 1-6, both curves overlap—the exponential model works! After hour 6, reality diverges as resources 
                become limited. The logistic model captures this with: N(t) = K / (1 + A·e<sup>-rt</sup>), where K is the 
                carrying capacity.
            </div>

            <div class="highlight">
                <strong>Key Lesson: Models Have Domains of Validity</strong><br>
                The exponential model isn't "wrong"—it's <em>approximately correct under specific conditions</em>. 
                Understanding when a model applies is as important as understanding the model itself. This is true throughout 
                science and engineering: every model has assumptions, and those assumptions limit where the model works.
            </div>

            <div class="highlight">
                <strong>Practice Problems:</strong><br>
                1. If a bacterial culture doubles every 20 minutes, starting with 100 bacteria, write the exponential growth function.<br>
                2. After 3 hours, how many bacteria would the exponential model predict? (Hint: 3 hours = 180 minutes = 9 doubling periods)<br>
                3. Sketch what happens to the function y = 2<sup>x</sup> when x is negative. Does it ever reach zero?<br>
                4. If ln(7) ≈ 1.95, what is e<sup>1.95</sup>?<br><br>
                
                <strong>Answers:</strong><br>
                1. N(t) = 100 · 2<sup>t/20</sup> where t is in minutes, or N(t) = 100 · e<sup>0.0347t</sup><br>
                2. N(180) = 100 · 2<sup>9</sup> = 100 · 512 = 51,200 bacteria<br>
                3. As x → -∞, 2<sup>x</sup> → 0 but never reaches it. For example: 2<sup>-3</sup> = 1/8 = 0.125, 2<sup>-10</sup> ≈ 0.00098<br>
                4. e<sup>1.95</sup> ≈ 7, because exponential and logarithm are inverse functions
            </div>
        </div>

        <div id="module3">
            <h1>Module 3: Survey of Science Applications</h1>

            <p>
                Exponential and logarithmic patterns appear throughout the natural sciences. This module surveys key applications 
                to show why these mathematical concepts are fundamental to understanding the physical world.
            </p>

            <h2>3.1 Chemistry: Radioactive Decay</h2>
            
            <div class="historical">
                <strong>The Discovery of Radioactivity and Its Mathematics:</strong>
                <ul>
                    <li><strong>1896:</strong> Henri Becquerel discovered radioactivity when uranium salts fogged photographic plates</li>
                    <li><strong>1898:</strong> Marie and Pierre Curie isolated radium and polonium, studying their properties intensively</li>
                    <li><strong>1900-1902:</strong> Ernest Rutherford and Frederick Soddy at McGill University in Montreal studied 
                    thorium decay products and made a crucial discovery</li>
                    <li><strong>1902:</strong> Rutherford and Soddy published their finding: radioactive decay follows an exponential pattern. 
                    The rate of decay is proportional to the amount present—a stunning insight.</li>
                    <li><strong>1903:</strong> Rutherford introduced the concept of "half-life"—the time for half the radioactive atoms to decay</li>
                    <li><strong>1904-1907:</strong> Rutherford used radioactive decay rates to estimate Earth's age at hundreds of millions of years, 
                    revolutionizing geology</li>
                    <li><strong>1946:</strong> Willard Libby developed radiocarbon dating using Carbon-14, for which he won the 1960 Nobel Prize</li>
                </ul>
                
                <strong>Before This Discovery:</strong> Scientists didn't understand what radioactivity was or how to predict it. 
                The exponential decay law allowed precise predictions and measurements. It revealed that atomic processes are statistical—
                you cannot predict when a specific atom will decay, but you can predict what fraction will decay in a given time.
                
                <strong>Impact:</strong> Radioactive decay mathematics enabled:
                <ul>
                    <li>Accurate dating of archaeological artifacts (radiocarbon) and geological formations (uranium-lead dating)</li>
                    <li>Medical isotope therapy—doctors could calculate exact doses needed</li>
                    <li>Nuclear energy calculations</li>
                    <li>Understanding nuclear physics and quantum mechanics</li>
                </ul>
            </div>

            <p>
                Each radioactive element has a characteristic <strong>half-life</strong> (symbol t<sub>½</sub>)—the time for half 
                the atoms to decay. The mathematical model is:
            </p>

            <div class="formula">
                N(t) = N<sub>0</sub> · e<sup>-λt</sup>
            </div>
            <p style="text-align: center; font-size: 0.95em; color: #666;">
                Where:<br>
                N(t) = number of radioactive atoms remaining at time t<br>
                N<sub>0</sub> = initial number of radioactive atoms (at t=0)<br>
                λ = decay constant (Greek letter lambda, a number specific to each isotope)<br>
                t = time elapsed<br>
                e ≈ 2.71828 (the natural exponential base)
            </p>

            <div class="example">
                <strong>Understanding the Decay Constant λ:</strong><br>
                The decay constant λ determines how fast the decay happens. It's related to half-life, and we can derive 
                this relationship explicitly.<br><br>
                
                <strong>Derivation:</strong> By definition, at the half-life t<sub>½</sub>, exactly half the original atoms remain. 
                So N(t<sub>½</sub>) = N<sub>0</sub>/2. Substituting into our decay equation:<br><br>
                
                N<sub>0</sub>/2 = N<sub>0</sub> · e<sup>-λt<sub>½</sub></sup><br><br>
                
                Divide both sides by N<sub>0</sub>:<br>
                1/2 = e<sup>-λt<sub>½</sub></sup><br><br>
                
                Take natural logarithm of both sides:<br>
                ln(1/2) = -λt<sub>½</sub><br><br>
                
                Since ln(1/2) = -ln(2):<br>
                -ln(2) = -λt<sub>½</sub><br><br>
                
                Therefore:<br>
                <div style="text-align: center; margin: 10px 0; font-size: 1.1em;">
                    <strong>λ = ln(2) / t<sub>½</sub> ≈ 0.693 / t<sub>½</sub></strong>
                </div>
                
                <ul>
                    <li>Large λ means fast decay (short half-life)</li>
                    <li>Small λ means slow decay (long half-life)</li>
                </ul>
                
                For Carbon-14: t<sub>½</sub> = 5,730 years, so λ = 0.693/5730 ≈ 0.000121 per year<br>
                For Uranium-238: t<sub>½</sub> = 4.5 billion years, so λ ≈ 0.000000000154 per year (very slow!)
            </div>

            <div class="graph-container">
                <div class="graph-title">Fig. 3.1-1: Carbon-14 Radioactive Decay</div>
                <div class="plotly-wrapper" style="position: relative; max-width: 700px; margin: 0 auto;">
                    <div id="carbon14Graph" class="plotly-graph" style="width:100%; height:400px;"></div>
                </div>
                <div class="graph-caption">
                    Carbon-14 half-life = 5,730 years. After one half-life, 50% remains. After two half-lives (11,460 years), 25% remains. 
                    Used for dating ancient organic materials up to ~50,000 years old.
                </div>
            </div>

            <div class="example">
                <strong>Radiocarbon Dating Example:</strong><br>
                An archaeologist finds a wooden artifact. Living wood has about 15.3 disintegrations per minute per gram of carbon. 
                The artifact shows 11.5 disintegrations per minute per gram.<br><br>
                
                <strong>Important Note on Measurement:</strong> We defined N(t) as the <em>number of radioactive atoms</em>, 
                but scientists actually measure the <em>disintegration rate</em> (activity). Why can we use rates instead 
                of atom counts?<br><br>
                
                The key insight: <strong>disintegration rate is directly proportional to the number of radioactive atoms present.</strong><br>
                Rate = k · N (where k is some constant)<br><br>
                
                Therefore:<br>
                Rate(t) / Rate<sub>0</sub> = (k · N(t)) / (k · N<sub>0</sub>) = N(t) / N<sub>0</sub><br><br>
                
                The constant k cancels out! This means we can substitute disintegration rates directly into our decay equation:
                <div style="text-align: center; margin: 10px 0;">
                    Rate(t) = Rate<sub>0</sub> · e<sup>-λt</sup>
                </div>
                
                Now we can solve:<br>
                11.5 = 15.3 · e<sup>-λt</sup><br>
                11.5/15.3 = e<sup>-λt</sup><br>
                0.752 = e<sup>-λt</sup><br>
                ln(0.752) = -λt<br>
                -0.285 = -λt<br>
                t = 0.285/λ = 0.285/0.000121 ≈ 2,350 years<br><br>
                
                The artifact is approximately 2,350 years old—dating to around 325 BCE!
            </div>

            <h2>3.2 Physics: Newton's Law of Cooling</h2>
            
            <div class="historical">
                <strong>Newton's Discovery of Cooling Laws:</strong>
                <ul>
                    <li><strong>~1701:</strong> Isaac Newton published anonymous work in <em>Philosophical Transactions</em> 
                    describing cooling experiments</li>
                    <li><strong>Before Newton:</strong> No mathematical description of how things cooled. It was empirically known 
                    that hot objects cool faster initially, but no quantitative law existed.</li>
                    <li><strong>Newton's Insight:</strong> The rate of cooling is proportional to the temperature difference between 
                    the object and its surroundings. This leads to exponential decay toward ambient temperature.</li>
                    <li><strong>1780s:</strong> Newton's law was refined and widely used in physics and engineering</li>
                    <li><strong>Modern Understanding:</strong> Newton's law is an approximation valid for natural convection cooling 
                    when temperature differences aren't too large. More complex physics (radiation, forced convection) requires 
                    modifications, but the basic exponential pattern holds.</li>
                </ul>
                
                <strong>Impact:</strong> Allowed engineers to predict cooling times for:
                <ul>
                    <li>Industrial processes (metallurgy, glass-making)</li>
                    <li>Food safety (knowing how long until food cools to safe temperatures)</li>
                    <li>Forensic science (estimating time of death from body temperature)</li>
                    <li>Climate science (understanding heat transfer in the atmosphere)</li>
                </ul>
            </div>

            <div class="formula">
                T(t) = T<sub>room</sub> + (T<sub>0</sub> - T<sub>room</sub>) · e<sup>-kt</sup>
            </div>
            <p style="text-align: center; font-size: 0.95em; color: #666;">
                Where:<br>
                T(t) = temperature at time t<br>
                T<sub>room</sub> = ambient room temperature (the temperature the object approaches)<br>
                T<sub>0</sub> = initial temperature at t=0<br>
                k = cooling constant (depends on object's properties: size, material, surface area)<br>
                t = time elapsed
            </p>

            <div class="example">
                <strong>Understanding the Cooling Constant k:</strong><br>
                The constant k determines how fast cooling happens. It depends on:
                <ul>
                    <li><strong>Surface area:</strong> More surface → faster cooling (larger k)</li>
                    <li><strong>Mass:</strong> More mass → slower cooling (smaller k)</li>
                    <li><strong>Material:</strong> Metals cool faster than ceramics (different k values)</li>
                    <li><strong>Environment:</strong> Windy conditions → faster cooling (larger k)</li>
                </ul>
                
                Typical k values for a coffee cup: k ≈ 0.05 to 0.1 per minute
            </div>

            <div class="graph-container">
                <div class="graph-title">Fig. 3.2-1: Coffee Cooling from 90°C to 20°C Room Temperature</div>
                <div class="plotly-wrapper" style="position: relative; max-width: 700px; margin: 0 auto;">
                    <div id="coffeeCoolingGraph" class="plotly-graph" style="width:100%; height:400px;"></div>
                </div>
                <div class="graph-caption">
                    The coffee asymptotically approaches room temperature but never quite reaches it mathematically 
                    (though practically it gets close enough after ~60 minutes).
                </div>
            </div>

            <h2>3.3 Pharmacology: Drug Elimination</h2>
            
            <div class="historical">
                <strong>Development of Pharmacokinetics:</strong>
                <ul>
                    <li><strong>Pre-1900:</strong> Doctors prescribed drugs based on experience and trial-and-error, with no 
                    mathematical understanding of how drugs left the body</li>
                    <li><strong>1913:</strong> Michaelis and Menten described enzyme kinetics, laying groundwork for understanding 
                    drug metabolism</li>
                    <li><strong>1920s-1930s:</strong> Researchers discovered that many drugs follow exponential elimination—the 
                    body clears a constant fraction per hour, not a constant amount</li>
                    <li><strong>1937:</strong> Torsten Teorell in Sweden published groundbreaking work on pharmacokinetics, 
                    including the concept of half-life for drugs</li>
                    <li><strong>1953:</strong> Friedrich Dost published <em>Der Blutspiegel</em>, systematizing pharmacokinetics 
                    as a mathematical discipline</li>
                    <li><strong>1960s-1970s:</strong> Pharmacokinetics became standard in drug development; regulatory agencies 
                    required half-life data for new drugs</li>
                </ul>
                
                <strong>Impact:</strong> Understanding exponential elimination allowed:
                <ul>
                    <li>Safe dosing schedules—knowing when to give next dose</li>
                    <li>Avoiding toxic accumulation</li>
                    <li>Predicting drug interactions</li>
                    <li>Designing slow-release medications</li>
                    <li>Understanding why some drugs need to be taken multiple times per day while others are once daily</li>
                </ul>
            </div>

            <p>
                Most drugs follow first-order elimination kinetics—exponential decay with a characteristic half-life. 
                This means the body eliminates a constant <em>fraction</em> of the drug per hour (not a constant <em>amount</em>).
            </p>

            <div class="formula">
                C(t) = C<sub>0</sub> · e<sup>-k<sub>e</sub>t</sup>
            </div>
            <p style="text-align: center; font-size: 0.95em; color: #666;">
                Where:<br>
                C(t) = drug concentration in blood at time t<br>
                C<sub>0</sub> = initial concentration (right after taking the drug)<br>
                k<sub>e</sub> = elimination rate constant (subscript e stands for "elimination")<br>
                t = time after taking the drug
            </p>

            <div class="example">
                <strong>Understanding k<sub>e</sub> and Half-Life:</strong><br>
                The elimination rate constant k<sub>e</sub> relates to half-life by the same mathematical relationship we derived 
                in section 3.1 for radioactive decay—it's really the same equation with different symbols:
                <div style="text-align: center; margin: 10px 0;">
                    t<sub>½</sub> = ln(2) / k<sub>e</sub> ≈ 0.693 / k<sub>e</sub>
                </div>
                
                This makes sense: whether atoms are decaying or drug molecules are being eliminated, the underlying mathematics 
                is identical—a constant fraction is removed per unit time.<br><br>
                
                If a drug has k<sub>e</sub> = 0.14 per hour:<br>
                t<sub>½</sub> = 0.693 / 0.14 ≈ 5 hours (this would be caffeine)<br><br>
                
                After one half-life (5 hours): 50% remains<br>
                After two half-lives (10 hours): 25% remains<br>
                After three half-lives (15 hours): 12.5% remains
            </div>

            <div class="example">
                <strong>Examples of Drug Half-Lives:</strong>
                <ul>
                    <li><strong>Aspirin:</strong> ~15 minutes. Eliminated very quickly. Why pain relief lasts longer is due 
                    to aspirin's lasting effects on enzymes, not the drug itself staying in your body.</li>
                    <li><strong>Ibuprofen:</strong> ~2 hours. This is why you need to take it every 4-6 hours for sustained effect.</li>
                    <li><strong>Caffeine:</strong> ~5 hours. Why that afternoon coffee affects your sleep—half is still in 
                    your system at bedtime!</li>
                    <li><strong>Diazepam (Valium):</strong> ~30-60 hours. Stays in your system for days. Can accumulate 
                    with repeated doses.</li>
                    <li><strong>Fluoxetine (Prozac):</strong> ~4-6 days. Extremely long half-life. Takes weeks to wash out 
                    completely when stopping the medication.</li>
                </ul>
                
                <strong>Dosing Rule:</strong> Doctors typically schedule next dose after 3-4 half-lives, when most of the 
                previous dose has been eliminated.
            </div>

            <h2>3.4 Astronomy and Magnitudes</h2>
            
            <div class="historical">
                <strong>The History of Stellar Magnitudes:</strong>
                <ul>
                    <li><strong>~150 BCE:</strong> Hipparchus created first star catalog, classifying stars by "magnitude" 
                    (brightness) from 1 (brightest) to 6 (faintest visible to naked eye)</li>
                    <li><strong>150 CE:</strong> Ptolemy refined the system in the <em>Almagest</em></li>
                    <li><strong>1610:</strong> Galileo's telescope revealed fainter stars. The magnitude system needed extension 
                    beyond magnitude 6.</li>
                    <li><strong>1856:</strong> Norman Pogson proposed making the magnitude scale precise: a difference of 5 
                    magnitudes equals exactly 100× brightness ratio. <strong>How does this lead to the result that 1 magnitude 
                    difference equals 100<sup>1/5</sup> ≈ 2.512× brightness?</strong><br><br>
                    
                    <em>Derivation:</em> Let's say each magnitude step multiplies brightness by some constant factor r. Then:
                    <ul style="margin: 5px 0; padding-left: 20px;">
                        <li>1 magnitude difference → brightness ratio = r</li>
                        <li>2 magnitude difference → brightness ratio = r × r = r<sup>2</sup></li>
                        <li>3 magnitude difference → brightness ratio = r<sup>3</sup></li>
                        <li>5 magnitude difference → brightness ratio = r<sup>5</sup></li>
                    </ul>
                    
                    But Pogson defined 5 magnitudes as exactly 100×, so:<br>
                    r<sup>5</sup> = 100<br>
                    Taking the fifth root: r = 100<sup>1/5</sup> ≈ 2.512<br><br>
                    
                    Therefore, magnitude 1 is exactly 100<sup>1/5</sup> ≈ 2.512× brighter than magnitude 2.</li>
                    <li><strong>1860s:</strong> Pogson's logarithmic scale was adopted internationally</li>
                    <li><strong>Modern era:</strong> Magnitudes extended to describe objects far beyond human vision—magnitude 
                    30 galaxies (trillions of times fainter than naked eye limit), and negative magnitudes for bright objects 
                    (Sun is magnitude -27)</li>
                </ul>
                
                <strong>Why Logarithmic?</strong> Human perception of brightness is approximately logarithmic (Weber-Fechner law, 
                1860s)—we perceive brightness ratios rather than absolute differences. A logarithmic magnitude scale matches 
                how we actually see.
            </div>

            <div class="highlight">
                <strong>The Fascinating Biology of Logarithmic Perception:</strong><br><br>
                
                It's remarkable that both human perception of brightness (vision) and sound loudness (hearing) are logarithmic. 
                This likely arose through evolution for very practical reasons:<br><br>
                
                <strong>Vision:</strong> Our eyes can perceive an enormous range of light intensities—from starlight (about 
                10<sup>-6</sup> lux) to bright sunlight (about 10<sup>5</sup> lux), spanning roughly 11 orders of magnitude. 
                If our perception were linear, either:
                <ul>
                    <li>We'd be blind in dim light (not sensitive enough), OR</li>
                    <li>We'd be damaged by bright light (too sensitive)</li>
                </ul>
                
                <strong>Hearing:</strong> Similarly, our ears can hear sounds from the threshold of hearing (10<sup>-12</sup> W/m²) 
                to the threshold of pain (1 W/m²)—a range of 12 orders of magnitude! The decibel scale (logarithmic) matches 
                our perception.<br><br>
                
                <strong>Evolutionary Advantage:</strong> Logarithmic perception allows us to:
                <ul>
                    <li><strong>Be sensitive at low levels:</strong> Detect faint sounds (predators) and see in dim light (night vision)</li>
                    <li><strong>Avoid damage at high levels:</strong> Not be overwhelmed or injured by loud sounds or bright light</li>
                    <li><strong>Perceive relative changes:</strong> Notice "twice as bright" or "twice as loud" regardless of 
                    absolute level—useful for comparing stimuli across many contexts</li>
                </ul>
                
                This is why astronomers chose a logarithmic magnitude scale—it naturally matches how humans actually perceive 
                brightness differences! A magnitude 2 star looks about as much brighter than magnitude 3 as magnitude 5 looks 
                brighter than magnitude 6, even though the absolute intensity differences are vastly different.
            </div>

            <p>
                Stellar magnitude m is defined logarithmically:
            </p>
            <div class="formula">
                m<sub>1</sub> - m<sub>2</sub> = -2.5 log<sub>10</sub>(I<sub>1</sub> / I<sub>2</sub>)
            </div>
            <p style="text-align: center; font-size: 0.95em; color: #666;">
                Where:<br>
                m = apparent magnitude (brightness as seen from Earth)<br>
                I = intensity (actual light energy received)<br>
                The factor -2.5 comes from Pogson's definition
            </p>

            <div class="example">
                <strong>Understanding Magnitudes:</strong>
                <ul>
                    <li><strong>Smaller magnitude = brighter</strong> (counterintuitive but historical)</li>
                    <li>1 magnitude difference = 2.512× brightness (because 2.512<sup>5</sup> ≈ 100)</li>
                    <li>5 magnitudes difference = 100× brightness</li>
                    <li>10 magnitudes difference = 10,000× brightness</li>
                </ul>
                
                Examples:
                <ul>
                    <li>Sun: magnitude -27</li>
                    <li>Full Moon: magnitude -13</li>
                    <li>Venus (brightest): magnitude -5</li>
                    <li>Sirius (brightest star): magnitude -1.5</li>
                    <li>Naked eye limit: magnitude +6</li>
                    <li>Hubble Space Telescope limit: magnitude +31</li>
                </ul>
                
                From magnitude +6 (naked eye) to +31 (Hubble) is 25 magnitudes = 10 billion times fainter!
            </div>

            <h2>3.5 Seismology: The Richter Scale</h2>
            
            <div class="historical">
                <strong>Measuring Earthquakes:</strong>
                <ul>
                    <li><strong>Pre-1935:</strong> Earthquakes described qualitatively (small, moderate, great) with no 
                    standardized scale. Impossible to compare earthquakes across regions.</li>
                    <li><strong>1935:</strong> Charles F. Richter at Caltech developed the Richter scale for Southern 
                    California earthquakes, using seismograph recordings</li>
                    <li><strong>Richter's Innovation:</strong> Used a logarithmic scale because earthquake energies span 
                    such enormous ranges—from barely detectable to devastating</li>
                    <li><strong>1950s-1960s:</strong> Richter scale extended worldwide and refined</li>
                    <li><strong>1970s:</strong> Moment magnitude scale (M<sub>w</sub>) developed to handle very large 
                    earthquakes better, but still logarithmic</li>
                    <li><strong>Today:</strong> "Magnitude" in news reports usually means moment magnitude, but the principle 
                    is the same—logarithmic scaling of energy</li>
                </ul>
                
                <strong>Why Logarithmic?</strong> Earthquake energies range from microjoules to 10<sup>18</sup> joules 
                (Great Chilean Earthquake, 1960)—a factor of 10<sup>24</sup>! A linear scale would be useless. Logarithms 
                compress this into a manageable 0-10 scale.
            </div>

            <p>
                Each integer increase in magnitude represents approximately 31.6× more energy release. Let's derive 
                the general formula:
            </p>

            <div class="example">
                <strong>Understanding the Energy Ratio:</strong><br>
                The Richter scale is defined so that each unit increase in magnitude corresponds to a 10× increase in 
                amplitude of seismic waves measured on a seismograph.<br><br>
                
                <strong>Why not simply 100× energy increase?</strong><br>
                You might expect: since energy is related to amplitude squared (energy ∝ amplitude<sup>2</sup>), and 
                1 magnitude = 10× amplitude, then 1 magnitude should equal (10)<sup>2</sup> = 100× energy. But the 
                actual relationship is <strong>10<sup>1.5</sup> ≈ 31.6× energy per magnitude</strong>—not 100×. Why?<br><br>
                
                <strong>The full story (beyond our scope):</strong><br>
                The complete derivation involves seismic wave physics including:
                <ul>
                    <li>How earthquake waves spread in three dimensions through the Earth</li>
                    <li>Wave attenuation (energy loss) as waves travel</li>
                    <li>The relationship between source energy and wave amplitude at various distances</li>
                    <li>Integration over the full seismic wave spectrum</li>
                </ul>
                
                These factors combine to give the empirically verified and theoretically derived result: 
                <strong>each 1-unit magnitude increase multiplies energy by 10<sup>1.5</sup> ≈ 31.6×</strong>.<br><br>
                
                <strong>Why this makes sense:</strong> The result (31.6×) falls between 10× (the amplitude increase) and 
                100× (naive amplitude-squared), which is reasonable given that multiple physical factors are involved. 
                The exponent 1.5 is a compromise between the linear amplitude scaling (exponent 1) and the squared-amplitude 
                energy relationship (exponent 2).<br><br>
                
                Therefore, for a difference of n magnitude units:<br>
                Energy ratio = (10<sup>1.5</sup>)<sup>n</sup> = 10<sup>1.5n</sup><br><br>
                
                For magnitudes M<sub>1</sub> and M<sub>2</sub>:
            </div>

            <div class="formula">
                Energy ratio = 10<sup>1.5(M<sub>1</sub> - M<sub>2</sub>)</sup>
            </div>

            <div class="example">
                <strong>Energy Differences:</strong>
                <ul>
                    <li>Magnitude 5 vs 4: 10<sup>1.5</sup> ≈ 31.6× more energy</li>
                    <li>Magnitude 6 vs 5: 10<sup>1.5</sup> ≈ 31.6× more energy</li>
                    <li>Magnitude 7 vs 5: 10<sup>3</sup> = 1,000× more energy</li>
                    <li>Magnitude 9 vs 7: 10<sup>3</sup> = 1,000× more energy (comparable to largest nuclear weapons)</li>
                </ul>
                
                A magnitude 9.0 earthquake (like 2011 Tōhoku, Japan) releases about 32,000× more energy than a magnitude 6.0! 
                (10<sup>1.5×(9-6)</sup> = 10<sup>4.5</sup> ≈ 32,000)
            </div>

            <div class="highlight">
                <strong>Module Summary:</strong><br>
                Exponential patterns emerge when <strong>rates of change are proportional to current amounts</strong>:
                <ul>
                    <li>Radioactive decay: decay rate ∝ atoms present</li>
                    <li>Newton's cooling: cooling rate ∝ temperature difference</li>
                    <li>Drug elimination: elimination rate ∝ drug concentration</li>
                </ul>
                
                Logarithms serve two key purposes: 
                <ul>
                    <li><strong>Compressing huge ranges</strong> into manageable scales (magnitudes, Richter, pH, decibels)</li>
                    <li><strong>Linearizing exponential relationships</strong> for easier analysis and computation</li>
                </ul>
                
                Understanding exponentials and logarithms is essential for quantitative work in any science.
            </div>

            <div class="highlight">
                <strong>Practice Problems:</strong><br>
                1. If a radioactive sample has a half-life of 8 days, what fraction remains after 24 days?<br>
                2. Coffee at 85°C is placed in a 20°C room. After 10 minutes it's 65°C. Estimate the temperature after 20 minutes.<br>
                3. A drug with a 4-hour half-life is taken at 100 mg. How much remains after 12 hours?<br>
                4. Two stars have magnitudes 3.0 and 5.0. How many times brighter is the magnitude 3.0 star?<br>
                5. An earthquake is magnitude 7.5. How much more energy does it release compared to a magnitude 6.0 quake?<br><br>
                
                <strong>Answers:</strong><br>
                1. 24 days = 3 half-lives, so (1/2)<sup>3</sup> = 1/8 = 12.5% remains<br>
                2. The temperature difference halved from 65°C to about (65-20)/2 ≈ 22.5°C above room temp, so T ≈ 42-43°C<br>
                3. 12 hours = 3 half-lives, so (1/2)<sup>3</sup> × 100 = 12.5 mg remains<br>
                4. Magnitude difference = 2, so brightness ratio = 2.512<sup>2</sup> ≈ 6.3× brighter<br>
                5. Magnitude difference = 1.5, so energy ratio = 10<sup>1.5×1.5</sup> = 10<sup>2.25</sup> ≈ 178× more energy
            </div>
        </div>

        <div id="module4">
            <h1>Module 4: Computing Deep Dive</h1>

            <div class="historical">
                <strong>The Digital Revolution and Binary Computing:</strong>
                <ul>
                    <li><strong>1936-1937:</strong> Alan Turing and Alonzo Church independently developed theories of computation, 
                    establishing theoretical foundations</li>
                    <li><strong>1937:</strong> Claude Shannon's master's thesis "A Symbolic Analysis of Relay and Switching Circuits" 
                    showed that Boolean algebra (binary logic) could represent any logical operation with electrical switches</li>
                    <li><strong>1945:</strong> John von Neumann described the stored-program computer architecture (still used today)</li>
                    <li><strong>1945-1950:</strong> First electronic digital computers built: ENIAC (1945), EDVAC (1949), 
                    EDSAC (1949). All used binary</li>
                    <li><strong>1950s-1960s:</strong> Computer science emerged as a discipline. Algorithm analysis became systematic.</li>
                    <li><strong>1962:</strong> Donald Knuth began work on <em>The Art of Computer Programming</em>, systematizing 
                    algorithm analysis using Big-O notation</li>
                    <li><strong>1970s:</strong> Complexity theory formalized (P vs NP question posed 1971). Logarithmic algorithms 
                    recognized as especially efficient.</li>
                </ul>
            </div>

            <h2>4.1 Why Base 2? Binary and Computing</h2>
            <p>
                Computers work in binary (base 2) because electronic circuits have two stable states: on (1) and off (0). 
                While you could theoretically build circuits with 10 stable states (base 10), reliably distinguishing between 
                10 different voltage levels is much harder than distinguishing between 2. Binary is:
            </p>
            <ul>
                <li><strong>Simple:</strong> Only two states to distinguish</li>
                <li><strong>Reliable:</strong> Noise less likely to cause errors</li>
                <li><strong>Fast:</strong> Quick switching between two states</li>
                <li><strong>Easy to manufacture:</strong> Transistors naturally have two states (conducting/non-conducting)</li>
            </ul>

            <p>
                Base-2 logarithms (written as log<sub>2</sub>(n) or sometimes lg(n)) are the natural choice for computer science. 
                They answer fundamental questions about binary systems:
            </p>
            
            <div class="highlight">
                <strong>Key Insight:</strong> log<sub>2</sub>(n) tells you:
                <ul>
                    <li>"How many times must I divide n by 2 to reach 1?"</li>
                    <li>"How many bits do I need to represent numbers from 0 to n?"</li>
                    <li>"How deep is a binary tree with n nodes?"</li>
                    <li>"How many times can I halve the problem size?"</li>
                </ul>
            </div>

            <div class="example">
                <strong>Bits Needed to Represent Numbers:</strong>
                <ul>
                    <li>Numbers 0-255: need ⌈log<sub>2</sub>(256)⌉ = 8 bits (1 byte)</li>
                    <li>Numbers 0-1023: need ⌈log<sub>2</sub>(1024)⌉ = 10 bits</li>
                    <li>Numbers 0-65535: need ⌈log<sub>2</sub>(65536)⌉ = 16 bits (2 bytes)</li>
                    <li>Numbers 0-4,294,967,295: need ⌈log<sub>2</sub>(2<sup>32</sup>)⌉ = 32 bits (4 bytes)</li>
                </ul>
                (The ceiling brackets ⌈ ⌉ mean "round up to the next integer")
            </div>

            <h2>4.2 Algorithm Analysis: Big-O Notation</h2>
            
            <div class="historical">
                <strong>Development of Algorithm Analysis:</strong>
                <ul>
                    <li><strong>1894:</strong> Paul Bachmann introduced the "O" notation in number theory</li>
                    <li><strong>1914:</strong> G.H. Hardy and J.E. Littlewood adapted it for asymptotic analysis</li>
                    <li><strong>1962-1976:</strong> Donald Knuth popularized Big-O notation for analyzing algorithms in 
                    <em>The Art of Computer Programming</em></li>
                    <li><strong>1970s:</strong> Big-O analysis became standard in computer science education</li>
                    <li><strong>Today:</strong> Every programmer learns Big-O notation—it's the universal language for discussing 
                    algorithm efficiency</li>
                </ul>
                
                <strong>Why This Matters:</strong> Before systematic complexity analysis, programmers didn't have a good way to 
                compare algorithms objectively. "This seems faster" was not quantitative. Big-O notation provides a mathematical 
                framework to predict how algorithms scale.
            </div>

            <p>
                Big-O notation describes how algorithm running time grows as input size n increases. It measures efficiency 
                independent of hardware, programming language, or implementation details. The notation focuses on the dominant 
                term for large n.
            </p>

            <div class="example">
                <strong>Common Complexity Classes:</strong>
                <ul>
                    <li><strong>O(1)</strong> - Constant time: Operation takes the same time regardless of input size. 
                    Example: accessing an array element by index.</li>
                    <li><strong>O(log n)</strong> - Logarithmic time: Execution time grows slowly, roughly proportional to 
                    log<sub>2</sub>(n). Example: binary search.</li>
                    <li><strong>O(n)</strong> - Linear time: Execution time grows directly with input size. 
                    Example: finding maximum in unsorted array.</li>
                    <li><strong>O(n log n)</strong> - Linearithmic time: Between linear and quadratic. 
                    Example: efficient sorting algorithms (mergesort, heapsort).</li>
                    <li><strong>O(n²)</strong> - Quadratic time: Execution time grows with square of input size. 
                    Example: simple sorting algorithms (bubble sort), nested loops.</li>
                    <li><strong>O(2<sup>n</sup>)</strong> - Exponential time: Grows extremely fast, impractical for large n. 
                    Example: brute-force solving traveling salesman problem.</li>
                </ul>
            </div>

            <div class="graph-container">
                <div class="graph-title">Fig. 4.2-1a: Algorithm Complexities — Small n (Linear Scale)</div>
                <div class="plotly-wrapper" style="position: relative; max-width: 700px; margin: 0 auto;">
                    <div id="algorithmGraphLinear" class="plotly-graph" style="width:100%; height:350px;"></div>
                </div>
                <div class="graph-caption">
                    For small input sizes (n = 1 to 20), all algorithms are fast. Note how O(n²) begins to separate 
                    from the others, while O(log n) stays remarkably flat.
                </div>
            </div>
            
            <div class="graph-container">
                <div class="graph-title">Fig. 4.2-1b: Algorithm Complexities — Large n (Semi-Log Scale)</div>
                <div class="plotly-wrapper" style="position: relative; max-width: 700px; margin: 0 auto;">
                    <div id="algorithmGraphLog" class="plotly-graph" style="width:100%; height:350px;"></div>
                </div>
                <div class="graph-caption">
                    Semi-log scale reveals the true story for n = 1 to 100: O(n²) explodes to 10,000 operations 
                    while O(log n) barely reaches 7. The logarithm function's slow growth is the key to efficient algorithms.
                </div>
            </div>

            <div class="example">
                <strong>Why O(log n) is Amazing:</strong><br>
                Consider searching in a sorted list of n = 1,000,000 items:
                <ul>
                    <li><strong>Linear search O(n):</strong> Worst case checks all 1,000,000 items</li>
                    <li><strong>Binary search O(log<sub>2</sub> n):</strong> Worst case checks only log<sub>2</sub>(1,000,000) 
                    ≈ 20 items!</li>
                </ul>
                
                That's 50,000× more efficient! And it only gets better for larger datasets:
                <ul>
                    <li>n = 1 billion: O(n) needs 1 billion operations, O(log n) needs only 30</li>
                    <li>n = 1 trillion: O(n) needs 1 trillion operations, O(log n) needs only 40</li>
                </ul>
                
                This is why binary search (repeatedly halving the search space) is so powerful.
            </div>

            <h2>4.3 Binary Search: The Classic O(log n) Algorithm</h2>
            
            <div class="historical">
                <strong>The Surprisingly Recent Invention of Binary Search:</strong>
                <ul>
                    <li><strong>Ancient:</strong> The idea of narrowing a search by halving appears in ancient puzzles and games</li>
                    <li><strong>1946:</strong> John Mauchly described binary search in notes about the ENIAC computer</li>
                    <li><strong>1957:</strong> First published correct binary search algorithm (many early versions had subtle bugs!)</li>
                    <li><strong>1960s:</strong> Binary search became a standard algorithm taught to all programmers</li>
                    <li><strong>2006:</strong> Joshua Bloch (Google) found a bug in many implementations that had existed for 
                    decades—integer overflow when calculating the midpoint!</li>
                </ul>
                
                <strong>Lesson:</strong> Even "simple" algorithms can be tricky to implement correctly. The correctness proof 
                for binary search, while straightforward, requires careful reasoning about loop invariants.
            </div>

            <p>
                Binary search works on sorted arrays by repeatedly dividing the search space in half. At each step, you compare 
                the target value with the middle element and eliminate half the remaining elements.
            </p>

            <div class="example">
                <strong>Binary Search Example:</strong><br>
                Search for 47 in sorted array: [3, 7, 12, 18, 23, 31, 47, 59, 68, 75, 82, 91, 99]<br><br>
                
                <strong>Step 1:</strong> Middle element is 31 (position 6 of 13)<br>
                47 > 31, so search right half: [47, 59, 68, 75, 82, 91, 99]<br><br>
                
                <strong>Step 2:</strong> Middle element is 75<br>
                47 < 75, so search left half: [47, 59, 68]<br><br>
                
                <strong>Step 3:</strong> Middle element is 59<br>
                47 < 59, so search left half: [47]<br><br>
                
                <strong>Step 4:</strong> Found 47!<br><br>
                
                Only 4 comparisons for an array of 13 elements! Linear search would need up to 7 comparisons (average 6.5).
                For large arrays, the difference becomes dramatic.
            </div>

            <div class="warning">
                <strong>Why Binary Search Requires Sorted Data:</strong><br>
                Binary search eliminates half the data at each step based on the comparison with the middle element. This only 
                works if you <em>know</em> which half contains your target—which requires the data to be sorted. If the array 
                is unsorted, you cannot determine which half to search, and you must check every element (linear search).
            </div>

            <h2>4.4 Binary Trees and Logarithmic Height</h2>
            
            <div class="historical">
                <strong>Development of Tree Data Structures:</strong>
                <ul>
                    <li><strong>1845:</strong> Arthur Cayley studied tree structures in mathematics (graph theory)</li>
                    <li><strong>1952:</strong> First computer science use of trees for sorting (sorting trees)</li>
                    <li><strong>1960:</strong> Binary search trees formalized</li>
                    <li><strong>1962:</strong> Adelson-Velsky and Landis invented AVL trees (self-balancing binary trees)</li>
                    <li><strong>1970s:</strong> B-trees invented for databases; red-black trees developed</li>
                    <li><strong>Today:</strong> Tree structures are fundamental to databases, file systems, compilers, 
                    and countless applications</li>
                </ul>
            </div>

            <p>
                A binary tree is a data structure where each node has at most two children (left and right). The height of the 
                tree—the longest path from root to any leaf—determines how many steps are needed to find any element.
            </p>

            <div class="highlight">
                <strong>Key Property:</strong> A balanced binary tree with n nodes has height approximately log<sub>2</sub>(n).
                <br><br>
                This means:
                <ul>
                    <li>1,000 nodes → height ≈ 10</li>
                    <li>1,000,000 nodes → height ≈ 20</li>
                    <li>1,000,000,000 nodes → height ≈ 30</li>
                </ul>
                
                Finding any element requires at most h comparisons, where h is the height. So searching in a balanced binary 
                tree is O(log n)! Let's see why below.
            </div>

            <div class="example">
                <strong>Why Trees Have Logarithmic Height:</strong><br>
                At each level of a complete binary tree:
                <ul>
                    <li>Level 0 (root): 1 node = 2<sup>0</sup></li>
                    <li>Level 1: 2 nodes = 2<sup>1</sup></li>
                    <li>Level 2: 4 nodes = 2<sup>2</sup></li>
                    <li>Level 3: 8 nodes = 2<sup>3</sup></li>
                    <li>Level h: 2<sup>h</sup> nodes</li>
                </ul>
                
                Total nodes in a complete binary tree of height h:
                <div style="text-align: center; margin: 10px 0;">
                    n = 2<sup>0</sup> + 2<sup>1</sup> + 2<sup>2</sup> + ... + 2<sup>h</sup> = 2<sup>h+1</sup> - 1
                </div>
                
                <strong>Solving for h:</strong><br>
                We have: n = 2<sup>h+1</sup> - 1<br><br>
                
                The symbol "≈" means "approximately equal." For any reasonable tree height, 2<sup>h+1</sup> is much larger 
                than 1. For example:
                <ul>
                    <li>If h = 5: 2<sup>6</sup> = 64, so 2<sup>6</sup> - 1 = 63 ≈ 64 (error less than 2%)</li>
                    <li>If h = 10: 2<sup>11</sup> = 2048, so 2<sup>11</sup> - 1 = 2047 ≈ 2048 (error less than 0.05%)</li>
                    <li>If h = 20: 2<sup>21</sup> > 2 million, so subtracting 1 is negligible</li>
                </ul>
                
                Therefore, we can approximate: <strong>n ≈ 2<sup>h+1</sup></strong> (the -1 becomes insignificant)<br><br>
                
                Taking log<sub>2</sub> of both sides:<br>
                log<sub>2</sub>(n) ≈ log<sub>2</sub>(2<sup>h+1</sup>)<br>
                log<sub>2</sub>(n) ≈ h + 1  (because log<sub>2</sub>(2<sup>h+1</sup>) = h + 1)<br>
                h ≈ log<sub>2</sub>(n) - 1<br><br>
                
                So the height grows logarithmically with the number of nodes!
            </div>

            <h2>4.5 Sorting Algorithms</h2>
            
            <div class="historical">
                <strong>History of Sorting:</strong>
                <ul>
                    <li><strong>Pre-computer:</strong> Sorting done by hand (alphabetizing card catalogs, organizing ledgers). 
                    Herman Hollerith's tabulating machines (1890s) could sort punched cards mechanically.</li>
                    <li><strong>1945:</strong> First computer sorting programs written for ENIAC</li>
                    <li><strong>1960:</strong> Tony Hoare invented Quicksort (average O(n log n))</li>
                    <li><strong>1962:</strong> Donald Shell invented Shellsort</li>
                    <li><strong>1970s:</strong> Heapsort developed (guaranteed O(n log n))</li>
                    <li><strong>1977:</strong> Timsort invented (used in Python and Java), combining mergesort and insertion sort</li>
                    <li><strong>1991:</strong> Proven that comparison-based sorting cannot be better than O(n log n) in the 
                    worst case—this is a fundamental limit!</li>
                </ul>
            </div>

            <p>
                Efficient sorting algorithms (mergesort, quicksort, heapsort) all have O(n log n) complexity. This is provably 
                optimal for comparison-based sorting—you cannot do better!
            </p>

            <div class="example">
                <strong>Why Sorting Requires O(n log n) Comparisons:</strong><br>
                Information theory provides an elegant proof. First, let's understand a key concept:
                <br><br>
                
                <strong>What is n! (n factorial)?</strong><br>
                The notation n! (read "n factorial") means the product of all integers from 1 to n:<br>
                <ul>
                    <li>3! = 3 × 2 × 1 = 6</li>
                    <li>4! = 4 × 3 × 2 × 1 = 24</li>
                    <li>5! = 5 × 4 × 3 × 2 × 1 = 120</li>
                    <li>10! = 10 × 9 × 8 × ... × 2 × 1 = 3,628,800</li>
                </ul>
                
                Factorials grow extremely fast! They count the number of different ways to arrange n items. For example, 
                3 items (A, B, C) can be arranged in 3! = 6 ways: ABC, ACB, BAC, BCA, CAB, CBA.<br><br>
                
                <strong>The proof (using information theory):</strong><br><br>
                
                <strong>Step 1:</strong> There are n! possible orderings of n items. Our sorting algorithm must figure out which 
                ordering is the correct one.<br><br>
                
                <strong>Step 2:</strong> Each comparison gives us one "bit" of information—a yes/no answer. For example, "Is A > B?" 
                gives us one bit.<br><br>
                
                <strong>Step 3:</strong> How many bits do we need to distinguish among n! possibilities?<br>
                This is a fundamental question in information theory. If you have M equally likely possibilities, you need 
                log<sub>2</sub>(M) bits to distinguish between them.<br><br>
                
                <em>Why?</em> Because with k bits, you can distinguish between 2<sup>k</sup> possibilities:
                <ul>
                    <li>1 bit distinguishes 2 possibilities (0 or 1)</li>
                    <li>2 bits distinguish 4 possibilities (00, 01, 10, 11)</li>
                    <li>3 bits distinguish 8 possibilities</li>
                    <li>k bits distinguish 2<sup>k</sup> possibilities</li>
                </ul>
                
                So if we need to distinguish M possibilities, we need k bits where 2<sup>k</sup> ≥ M. Taking log<sub>2</sub> 
                of both sides: k ≥ log<sub>2</sub>(M).<br><br>
                
                <strong>Step 4:</strong> For sorting, M = n!, so we need at least log<sub>2</sub>(n!) comparisons.<br><br>
                
                <strong>Step 5:</strong> How big is log<sub>2</sub>(n!)? This is where <strong>Stirling's approximation</strong> 
                comes in. Stirling's approximation is a mathematical formula that tells us:
                <div style="text-align: center; margin: 10px 0;">
                    n! ≈ √(2πn) · (n/e)<sup>n</sup>
                </div>
                
                Taking log<sub>2</sub> of both sides and simplifying (the details are technical), we get:
                <div style="text-align: center; margin: 10px 0;">
                    log<sub>2</sub>(n!) ≈ n log<sub>2</sub>(n) - n log<sub>2</sub>(e) + O(log n)
                </div>
                
                The dominant term is n log<sub>2</sub>(n), so: <strong>log<sub>2</sub>(n!) ≈ n log<sub>2</sub>(n)</strong>
                <br><br>
                
                Therefore, any comparison-based sorting algorithm must make at least O(n log n) comparisons in the worst case. 
                This is a fundamental limit—no algorithm can beat it!
            </div>

            <div class="warning">
                <strong>Why Bubble Sort is Bad:</strong><br>
                
                <strong>What is Bubble Sort?</strong><br>
                Bubble sort is a simple sorting algorithm that repeatedly steps through the list, compares adjacent elements, 
                and swaps them if they're in the wrong order. The algorithm gets its name because smaller elements "bubble" 
                to the top of the list.<br><br>
                
                <em>Example with [5, 2, 8, 1]:</em><br>
                <ul>
                    <li>Pass 1: Compare 5&2 (swap→[2,5,8,1]), compare 5&8 (no swap), compare 8&1 (swap→[2,5,1,8])</li>
                    <li>Pass 2: Compare 2&5 (no swap), compare 5&1 (swap→[2,1,5,8]), compare 5&8 (no swap)</li>
                    <li>Pass 3: Compare 2&1 (swap→[1,2,5,8]), compare 2&5 (no swap), compare 5&8 (no swap)</li>
                    <li>Now sorted: [1,2,5,8]</li>
                </ul>
                
                In the worst case, bubble sort must make about n×(n-1)/2 comparisons, which grows as n². <em>(Showing this 
                precisely is beyond our scope here, but the key is that for each of n elements, we potentially compare it 
                with n other elements, giving roughly n² comparisons.)</em><br><br>
                
                <strong>Bubble sort has O(n²) complexity. Why is this bad for large n?</strong><br>
                For n = 1,000 items:
                <ul>
                    <li>Bubble sort: ~1,000,000 comparisons</li>
                    <li>Mergesort: ~10,000 comparisons (100× faster!)</li>
                </ul>
                
                For n = 10,000 items:
                <ul>
                    <li>Bubble sort: ~100,000,000 comparisons</li>
                    <li>Mergesort: ~130,000 comparisons (770× faster!)</li>
                </ul>
                
                This is why bubble sort is taught as an example of what <em>not</em> to do for large datasets.
            </div>

            <div class="warning">
                <strong>Note on Sections 4.6 and 4.7:</strong><br>
                The following two sections touch on more advanced topics that are beyond the core scope of this lesson 
                on exponentials and logarithms. They provide perspective on how logarithmic complexity appears in many 
                important algorithms and in information theory, but they are not essential for understanding the fundamentals. 
                Feel free to skim or skip if you prefer to focus on the basics.
            </div>

            <h2>4.6 Divide and Conquer: The Power of Halving</h2>
            
            <p>
                Many efficient algorithms follow the "divide and conquer" strategy: break the problem into smaller subproblems, 
                solve them recursively, and combine results. When you repeatedly divide by 2, logarithms appear naturally.
            </p>

            <div class="example">
                <strong>Classic Divide and Conquer Algorithms (All O(n log n) or better):</strong><br>
                It would be too ambitious to discuss all these algorithms in detail here, but they are all important 
                precisely because their computational complexity grows as O(n log n) or better—making them practical 
                for large datasets:
                <ul>
                    <li><strong>Mergesort:</strong> Divide array in half, sort each half recursively, merge sorted halves. 
                    O(n log n)</li>
                    <li><strong>Quicksort:</strong> Pick pivot, partition around it, recursively sort partitions. 
                    Average O(n log n)</li>
                    <li><strong>Fast Fourier Transform (FFT):</strong> Revolutionized signal processing. Reduces O(n²) 
                    operation to O(n log n)</li>
                    <li><strong>Karatsuba multiplication:</strong> Fast multiplication of large numbers. O(n<sup>1.58</sup>) 
                    vs O(n²) for standard multiplication</li>
                </ul>
                
                The common thread: all leverage the power of repeatedly dividing problems in half, which naturally 
                introduces logarithmic factors.
            </div>

            <h2>4.7 Information Theory: Bits and Entropy</h2>
            
            <div class="historical">
                <strong>Claude Shannon and the Birth of Information Theory:</strong>
                <ul>
                    <li><strong>1948:</strong> Claude Shannon published "A Mathematical Theory of Communication" in 
                    <em>Bell System Technical Journal</em>—one of the most influential papers of the 20th century</li>
                    <li><strong>Shannon's Insight:</strong> Information can be quantified mathematically using logarithms. 
                    The fundamental unit is the bit (binary digit).</li>
                    <li><strong>Key Contributions:</strong>
                        <ul>
                            <li>Defined information entropy using logarithms</li>
                            <li>Proved channel capacity theorems</li>
                            <li>Showed optimal data compression rates</li>
                            <li>Established error-correcting code limits</li>
                        </ul>
                    </li>
                    <li><strong>Impact:</strong> Shannon's theory made possible: digital communications, the Internet, 
                    MP3 compression, mobile phones, satellite communications, data storage—essentially all modern digital 
                    technology.</li>
                </ul>
            </div>

            <p>
                Shannon defined the information content (in bits) of an event with probability p as:
            </p>
            
            <div class="formula">
                I(p) = -log<sub>2</sub>(p) = log<sub>2</sub>(1/p)
            </div>
            <p style="text-align: center; font-size: 0.95em; color: #666;">
                Where:<br>
                I(p) = information content in bits<br>
                p = probability of the event occurring (a number between 0 and 1)<br>
                The negative sign makes I positive since p < 1
            </p>

            <div class="example">
                <strong>Information Content Examples:</strong>
                <ul>
                    <li>Certain event (p = 1): I = log<sub>2</sub>(1) = 0 bits. No surprise, no information.</li>
                    <li>Fair coin flip (p = 0.5): I = log<sub>2</sub>(2) = 1 bit. This defines the bit!</li>
                    <li>Rolling a 6 on a fair die (p = 1/6): I = log<sub>2</sub>(6) ≈ 2.58 bits</li>
                    <li>Rare event (p = 1/1000): I = log<sub>2</sub>(1000) ≈ 9.97 bits. Surprising events carry more information.</li>
                </ul>
                
                <strong>Intuition:</strong> Rare events (small p) are surprising and carry more information. Common events 
                (large p) are unsurprising and carry less information. The logarithm quantifies this precisely.
            </div>

            <div class="highlight">
                <strong>Entropy: Average Information Content</strong><br>
                For a source producing messages with probabilities p<sub>1</sub>, p<sub>2</sub>, ..., p<sub>n</sub>, 
                the entropy H (average information per message) is:
                <div style="text-align: center; margin: 10px 0;">
                    H = -∑ p<sub>i</sub> log<sub>2</sub>(p<sub>i</sub>)
                </div>
                
                This tells you the average number of bits needed to encode messages from this source optimally. Shannon proved 
                you cannot compress data below its entropy without losing information.
            </div>

            <div class="example">
                <strong>Practical Application: Data Compression</strong><br>
                English text has entropy around 1.0-1.5 bits per character (not 8 bits/character as ASCII uses). This is why 
                compression works! Algorithms like Huffman coding (1952) and arithmetic coding use logarithms to achieve 
                near-optimal compression by assigning shorter codes to common characters.
                
                <ul>
                    <li>Common letter 'e' (p ≈ 0.127): needs log<sub>2</sub>(1/0.127) ≈ 3 bits in optimal coding</li>
                    <li>Rare letter 'z' (p ≈ 0.001): needs log<sub>2</sub>(1/0.001) ≈ 10 bits in optimal coding</li>
                </ul>
            </div>

            <div class="highlight">
                <strong>Module Summary</strong><br>
                Logarithms (especially base 2) are fundamental to computer science because:
                <ul>
                    <li><strong>Efficiency:</strong> O(log n) algorithms scale beautifully—doubling problem size adds only 
                    one more step</li>
                    <li><strong>Data structures:</strong> Balanced trees have logarithmic height, enabling fast search, 
                    insert, delete</li>
                    <li><strong>Information theory:</strong> Bits, entropy, and compression all defined using logarithms</li>
                    <li><strong>Problem solving:</strong> Divide-and-conquer strategies naturally produce logarithmic factors</li>
                </ul>
                
                Understanding logarithms is essential for analyzing algorithms, designing data structures, and reasoning about 
                computational efficiency.
            </div>

            <div class="highlight">
                <strong>Practice Problems:</strong><br>
                1. How many bits are needed to represent 10,000 different values?<br>
                2. A balanced binary search tree contains 1 million nodes. What is its maximum height?<br>
                3. If an algorithm has complexity O(n log n), how much longer does it take when n doubles?<br>
                4. What's the information content (in bits) of drawing a specific card from a 52-card deck?<br>
                5. A binary search on an array of size n = 512 finds an element. What's the maximum number of comparisons needed?<br><br>
                
                <strong>Answers:</strong><br>
                1. ⌈log<sub>2</sub>(10000)⌉ = ⌈13.29⌉ = 14 bits<br>
                2. Maximum height ≈ log<sub>2</sub>(1,000,000) ≈ 20 levels<br>
                3. If n → 2n, then n log n → 2n log(2n) = 2n(log n + 1) ≈ 2n log n (roughly doubles, slightly more)<br>
                4. I = log<sub>2</sub>(52) ≈ 5.7 bits<br>
                5. Maximum comparisons = ⌈log<sub>2</sub>(512)⌉ = 9
            </div>
        </div>

        <div id="module5">
            <h1>Module 5: Modern CS Applications</h1>

            <div class="historical">
                <strong>The Growth of Modern Computing:</strong>
                <ul>
                    <li><strong>1970s:</strong> Early computer science focused on algorithms and data structures</li>
                    <li><strong>1980s:</strong> Database systems and networking became major applications</li>
                    <li><strong>1990s:</strong> The Internet exploded; cryptography became essential for security</li>
                    <li><strong>2000s:</strong> Big data, cloud computing, and social networks emerged</li>
                    <li><strong>2010s:</strong> Machine learning and AI became mainstream; exponential growth in data</li>
                    <li><strong>2020s:</strong> Large language models, cryptocurrencies, quantum computing research</li>
                </ul>
            </div>

            <h2>5.1 Database Indexing</h2>
            
            <div class="historical">
                <strong>Evolution of Database Technology:</strong>
                <ul>
                    <li><strong>1960s:</strong> Early databases stored data sequentially. Finding a record required scanning 
                    through the entire file—O(n) time.</li>
                    <li><strong>1970:</strong> E.F. Codd proposed the relational database model</li>
                    <li><strong>1970s:</strong> B-trees invented by Rudolf Bayer and Ed McCreight (1972) at Boeing. Enabled 
                    efficient disk-based indexing with O(log n) access time.</li>
                    <li><strong>1979:</strong> Oracle database released commercially</li>
                    <li><strong>1980s-present:</strong> B-trees and their variants (B+ trees, B* trees) became the standard 
                    for database indexes</li>
                </ul>
                
                <strong>Why This Mattered:</strong> B-trees made databases scalable. A database with 1 billion records can find 
                any record in about 4-5 disk reads (logarithmic depth). Without indexing, you'd need to read millions of records.
            </div>

            <p>
                Modern databases use <strong>B-trees</strong> and <strong>B+ trees</strong> for indexing. To understand what a B-tree is, 
                think of it as an organized filing system:
            </p>
            
            <div class="example">
                <strong>What is a B-tree?</strong><br><br>
                
                Imagine you have a million customer records to store. How do you find "Smith, John" quickly?
                
                <p><strong>A Simple Tree (like a family tree):</strong> Each "node" or box in the tree contains one name and points 
                to two children. To find a name, start at the top and work down, going left if your target is smaller, right if larger. 
                With a million records, you'd need to check about 20 levels (since 2<sup>20</sup> ≈ 1 million). That's already pretty good!</p>
                
                <p><strong>A B-tree (Better for databases):</strong> Instead of each box holding just one item and having two children, 
                each box holds <em>many</em> items (typically 100-200) and can have many children (up to 200 or more). Think of each box 
                like a file folder that contains a whole sorted list of names and pointers to other folders.</p>
                
                <p><strong>Why is this better?</strong></p>
                <ul>
                    <li><strong>Fewer levels:</strong> With 100 items per node, you only need log<sub>100</sub>(1,000,000) ≈ 3 levels instead of 20!</li>
                    <li><strong>Matches disk storage:</strong> Reading from a disk is slow, but once you read a "block" you get lots of data at once. 
                    B-trees are designed so each node = one disk block read.</li>
                    <li><strong>Stays balanced automatically:</strong> When you add or remove data, the B-tree reorganizes itself to keep 
                    all paths from top to bottom roughly the same length. This keeps searches fast—no matter which record you're looking for, 
                    it takes the same number of steps.</li>
                </ul>
                
                <p><strong>The Key Point:</strong> B-trees let databases search efficiently because the height of the tree grows logarithmically. 
                Even with billions of records, you only need 4-5 steps to find any specific record. This is mentioned here as an important 
                real-world algorithm where understanding O(log n) growth is essential.</p>
            </div>

            <div class="example">
                <strong>Database Search Performance:</strong><br>
                Consider a database table with 100 million rows:
                <ul>
                    <li><strong>Sequential scan (no index):</strong> Must check all 100 million rows. At 1000 rows/second, 
                    takes 100,000 seconds ≈ 28 hours!</li>
                    <li><strong>B-tree index:</strong> Height ≈ log<sub>100</sub>(100,000,000) ≈ 4 levels (B-trees typically 
                    have fanout ~100). Find record in 4 disk reads ≈ 0.04 seconds—a million times faster!</li>
                </ul>
            </div>

            <h2>5.2 Network Routing and Distributed Systems</h2>
            
            <div class="historical">
                <strong>The Growth of Computer Networks:</strong>
                <ul>
                    <li><strong>1969:</strong> ARPANET, precursor to Internet, connected 4 computers</li>
                    <li><strong>1970s:</strong> Routing algorithms developed; spanning tree protocols for Ethernet</li>
                    <li><strong>1980s:</strong> TCP/IP standardized; DNS invented (uses hierarchy ≈ tree structure)</li>
                    <li><strong>1990s:</strong> Internet explodes; Google founded (1998) using PageRank algorithm</li>
                    <li><strong>2000s:</strong> Cloud computing emerges; distributed hash tables (DHT) for peer-to-peer networks</li>
                </ul>
            </div>

            <p>
                Many network protocols and distributed systems rely on logarithmic structures:
            </p>

            <ul>
                <li><strong>Routing tables:</strong> Use tree structures for O(log n) lookup of network routes</li>
                <li><strong>Distributed Hash Tables (DHT):</strong> Imagine 1 million computers connected peer-to-peer (like early file 
                sharing networks). How does one computer find which peer has a specific file? DHTs organize peers so any computer can be 
                found in only log₂(1,000,000) ≈ 20 "hops" through the network, even though there are a million nodes! Each computer only 
                needs to remember about 20 other computers' addresses—not all million.</li>
                <li><strong>Gossip protocols:</strong> Information spreads exponentially—reaching n nodes in O(log n) rounds</li>
                <li><strong>Load balancing trees:</strong> Distribute work across servers using tree structures</li>
            </ul>

            <h2>5.3 Exponential Growth in Technology</h2>
            
            <div class="historical">
                <strong>Moore's Law and Exponential Progress:</strong>
                <ul>
                    <li><strong>1965:</strong> Gordon Moore (Intel co-founder) observed that transistor count on integrated 
                    circuits doubled approximately every two years</li>
                    <li><strong>1965-2010s:</strong> Moore's Law held remarkably well. Computing power per dollar increased 
                    exponentially for 50+ years.</li>
                    <li><strong>Result:</strong> Smartphones today are millions of times more powerful than 1960s supercomputers</li>
                    <li><strong>2010s-present:</strong> Physical limits slowing Moore's Law, but exponential growth continues 
                    through other means (parallelism, specialized hardware, better algorithms)</li>
                </ul>
                
                <strong>Understanding Exponential Growth:</strong> If something doubles every 2 years, it grows by 1000× in 
                20 years, 1,000,000× in 40 years. This is why technology that was impossible in 1980 is commonplace today.
            </div>

            <div class="example">
                <strong>Exponential Growth Examples in Tech:</strong>
                <ul>
                    <li><strong>Storage costs:</strong> In 1980, 1 MB cost ~$200. Today, 1 TB (1 million MB) costs ~$20—
                    that's a 10 million-fold decrease!</li>
                    <li><strong>Internet traffic:</strong> Global IP traffic doubles roughly every 2-3 years</li>
                    <li><strong>AI training:</strong> Computation used to train largest AI models doubles every 6 months 
                    (2012-2020)—much faster than Moore's Law!</li>
                </ul>
            </div>

            <div class="highlight">
                <strong>Module Summary:</strong><br>
                Modern computer science leverages exponentials and logarithms in:
                <ul>
                    <li><strong>Databases:</strong> Logarithmic indexes enable scaling to billions of records</li>
                    <li><strong>Networks:</strong> Distributed systems use logarithmic structures for efficiency</li>
                    <li><strong>Growth patterns:</strong> Understanding exponential growth explains technological progress</li>
                </ul>
            </div>
        </div>

        <div id="module6">
            <h1>Module 6: AI and Machine Learning</h1>

            <div class="historical">
                <strong>The AI Revolution:</strong>
                <ul>
                    <li><strong>1943:</strong> McCulloch-Pitts artificial neuron model</li>
                    <li><strong>1956:</strong> "Artificial Intelligence" term coined at Dartmouth Conference</li>
                    <li><strong>1958:</strong> Perceptron (early neural network) invented by Frank Rosenblatt</li>
                    <li><strong>1986:</strong> Backpropagation algorithm popularized, enabling training of deeper networks</li>
                    <li><strong>2006:</strong> "Deep learning" renaissance begins (Geoffrey Hinton and others)</li>
                    <li><strong>2012:</strong> AlexNet wins ImageNet competition, proving deep learning's power</li>
                    <li><strong>2017:</strong> Transformer architecture invented (attention mechanisms)</li>
                    <li><strong>2020s:</strong> Large language models (GPT, Claude) with billions of parameters</li>
                </ul>
                
                <strong>Role of Mathematics:</strong> Throughout this history, logarithms and exponentials have been central to:
                <ul>
                    <li>Activation functions in neural networks</li>
                    <li>Loss functions for training</li>
                    <li>Probability calculations</li>
                    <li>Optimization algorithms</li>
                </ul>
            </div>

            <h2>6.1 Neural Networks and Activation Functions</h2>
            
            <p>
                Neural networks transform inputs through layers of neurons. Each neuron applies a non-linear activation function. 
                Common activation functions involve exponentials:
            </p>

            <h3>6.1.1 Sigmoid Function</h3>
            <div class="formula">
                σ(x) = 1 / (1 + e<sup>-x</sup>)
            </div>
            <p style="text-align: center; font-size: 0.95em; color: #666;">
                Where:<br>
                σ (sigma) = sigmoid function output (between 0 and 1)<br>
                x = input value<br>
                e ≈ 2.71828
            </p>

            <div class="example">
                <strong>Why Sigmoid?</strong>
                <ul>
                    <li>Maps any input to range (0, 1)—useful for representing probabilities</li>
                    <li>Smooth and differentiable—needed for backpropagation (the training algorithm)</li>
                    <li>When x is large positive: σ(x) ≈ 1</li>
                    <li>When x is large negative: σ(x) ≈ 0</li>
                    <li>When x = 0: σ(0) = 0.5 (middle of range)</li>
                </ul>
                
                <strong>Historical Note:</strong> Sigmoid was standard in 1990s-2000s neural networks. Today, ReLU and its 
                variants are more common for hidden layers, but sigmoid still used for output layers in binary classification.
            </div>

            <div class="graph-container">
                <div class="graph-title">Fig. 6.1-1: Sigmoid Function: σ(x) = 1/(1 + e<sup>−x</sup>)</div>
                <div class="plotly-wrapper" style="position: relative; max-width: 700px; margin: 0 auto;">
                    <div id="sigmoidGraph" class="plotly-graph" style="width:100%; height:400px;"></div>
                </div>
                <div class="graph-caption">
                    The sigmoid function maps any input to the range (0,1), making it useful for probabilities. 
                    Note key points: σ(0) = 0.5, and the function approaches (but never reaches) 0 and 1 as x → −∞ and +∞.
                </div>
            </div>

            <h3>6.1.2 Softmax Function</h3>
            <p>
                For multi-class classification (choosing among k categories), neural networks use softmax:
            </p>
            <div class="formula">
                softmax(x<sub>i</sub>) = e<sup>x<sub>i</sub></sup> / Σ e<sup>x<sub>j</sub></sup>
            </div>
            <p style="text-align: center; font-size: 0.95em; color: #666;">
                Where:<br>
                x<sub>i</sub> = score for class i (called a "logit")<br>
                The sum Σ is over all k classes<br>
                Output: probability distribution over classes (all probabilities sum to 1)
            </p>

            <div class="example">
                <strong>Softmax Example:</strong><br>
                Image classifier outputs scores for 3 categories: [2.0, 1.0, 0.1]<br><br>
                
                Apply softmax:
                <ul>
                    <li>e<sup>2.0</sup> ≈ 7.39</li>
                    <li>e<sup>1.0</sup> ≈ 2.72</li>
                    <li>e<sup>0.1</sup> ≈ 1.11</li>
                    <li>Sum = 7.39 + 2.72 + 1.11 ≈ 11.22</li>
                </ul>
                
                Probabilities:
                <ul>
                    <li>P(class 1) = 7.39/11.22 ≈ 0.66 (66%)</li>
                    <li>P(class 2) = 2.72/11.22 ≈ 0.24 (24%)</li>
                    <li>P(class 3) = 1.11/11.22 ≈ 0.10 (10%)</li>
                </ul>
                
                The exponential amplifies differences—larger scores get much higher probabilities.
            </div>

            <div class="graph-container">
                <div class="graph-title">Fig. 6.1-2: Softmax Function: How Varying One Input Affects Probabilities</div>
                <div class="plotly-wrapper" style="position: relative; max-width: 700px; margin: 0 auto;">
                    <div id="softmaxGraph" class="plotly-graph" style="width:100%; height:400px;"></div>
                </div>
                <div class="graph-caption">
                    For a 3-class softmax with inputs [x, 1.0, 0.5], this shows how the probability of each class changes as we vary x. 
                    Note how increasing x makes its class more probable (blue curve rising) while decreasing the probabilities of the other classes.
                    All three curves always sum to 1 (100% probability total).
                </div>
            </div>

            <h2>6.2 Loss Functions: Cross-Entropy</h2>
            
            <div class="historical">
                <strong>Development of Loss Functions:</strong>
                <ul>
                    <li><strong>1950s:</strong> Early ML used simple squared error loss</li>
                    <li><strong>1960s-1970s:</strong> Maximum likelihood estimation became standard in statistics</li>
                    <li><strong>1980s:</strong> Cross-entropy loss recognized as better for classification than squared error</li>
                    <li><strong>Connection to Information Theory:</strong> Cross-entropy has deep roots in Shannon's 
                    information theory (1948)—it measures how well a probability distribution matches the true distribution</li>
                </ul>
            </div>

            <p>
                Training neural networks requires a loss function that measures prediction error. For classification, 
                cross-entropy loss is standard:
            </p>

            <div class="formula">
                Loss = -Σ y<sub>i</sub> log(ŷ<sub>i</sub>)
            </div>
            <p style="text-align: center; font-size: 0.95em; color: #666;">
                Where:<br>
                y<sub>i</sub> = true probability for class i (usually 1 for correct class, 0 for others)<br>
                Å·<sub>i</sub> (y-hat) = predicted probability for class i<br>
                log = typically natural logarithm ln
            </p>

            <div class="example">
                <strong>Why Cross-Entropy?</strong><br>
                Consider predicting a dog photo (true class):
                <ul>
                    <li>Model predicts dog: 90%, cat: 10%<br>
                        Loss = -[1·log(0.9) + 0·log(0.1)] = -log(0.9) ≈ 0.11 (low loss—good!)</li>
                    <li>Model predicts dog: 10%, cat: 90%<br>
                        Loss = -log(0.1) ≈ 2.30 (high loss—bad!)</li>
                </ul>
                
                The logarithm heavily penalizes confident wrong predictions. If model assigns 0.01% to the correct class, 
                loss = -log(0.0001) ≈ 9.21—very large, forcing the model to fix this mistake.
            </div>

            <div class="graph-container">
                <div class="graph-title">Fig. 6.2-1: Cross-Entropy Loss: Loss = -log(p) for Binary Classification</div>
                <div class="plotly-wrapper" style="position: relative; max-width: 700px; margin: 0 auto;">
                    <div id="crossEntropyGraph" class="plotly-graph" style="width:100%; height:400px;"></div>
                </div>
                <div class="graph-caption">
                    This shows the loss when the true label is 1 (positive class). When the model predicts p = 1 (100% confident, correct), 
                    loss = 0. As predicted probability decreases, loss increases. When p approaches 0 (confidently wrong), loss → ∞, 
                    heavily penalizing incorrect confident predictions.
                </div>
            </div>

            <h2>6.3 Gradient Descent and Optimization</h2>
            
            <p>
                Training neural networks uses gradient descent: iteratively adjust weights to minimize loss. The learning rate 
                (how big a step to take) is often scheduled logarithmically or exponentially:
            </p>

            <ul>
                <li><strong>Exponential decay:</strong> learning_rate = initial_rate × e<sup>-kt</sup>, where t is training step</li>
                <li><strong>Step decay:</strong> Reduce learning rate by factor of 10 (logarithmic) every N epochs</li>
                <li><strong>Logarithmic schedules:</strong> learning_rate proportional to 1/log(1+t)</li>
            </ul>

            <div class="warning">
                <strong>Why Scheduling Matters:</strong><br>
                Early in training, you want large steps to quickly reach good regions of parameter space. Late in training, 
                you want small steps to fine-tune without overshooting. Exponential/logarithmic schedules provide this 
                automatically—fast initial progress, gradual refinement.
            </div>

            <h2>6.4 Logarithmic Scaling in Deep Learning</h2>
            
            <p>
                Modern deep learning involves logarithmic thinking in several ways:
            </p>

            <h3>6.4.1 Log-Uniform Sampling for Hyperparameters</h3>
            <p>
                When choosing learning rates, it's better to sample logarithmically: try 0.001, 0.01, 0.1, 1.0 (powers of 10) 
                rather than 0.1, 0.2, 0.3, 0.4. This is because learning rate spans many orders of magnitude.
            </p>

            <h3>6.4.2 Log-Scale Plotting</h3>
            <p>
                Training curves (loss vs. time) are often plotted on log scale. This reveals:
                <ul>
                    <li>Power-law relationships (linear on log-log plot)</li>
                    <li>Exponential convergence (linear on semi-log plot)</li>
                    <li>Plateaus and phase transitions</li>
                </ul>
            </p>

            <h3>6.4.3 Model Scaling Laws</h3>
            <div class="historical">
                <strong>Discovery of Neural Scaling Laws:</strong>
                <ul>
                    <li><strong>2020:</strong> Researchers at OpenAI discovered that model performance follows predictable 
                    power laws with respect to:
                        <ul>
                            <li>Model size (number of parameters)</li>
                            <li>Dataset size</li>
                            <li>Amount of compute</li>
                        </ul>
                    </li>
                    <li><strong>Key Finding:</strong> Performance improves as power law: loss ∝ N<sup>-α</sup> where N is 
                    model size and α ≈ 0.07-0.10 for language models</li>
                    <li><strong>Implication:</strong> Taking logarithms linearizes these relationships, making them predictable. 
                    On a log-log plot, performance vs. size is a straight line!</li>
                </ul>
            </div>

            <div class="example">
                <strong>Why Large Language Models Keep Growing:</strong><br>
                The scaling laws show that doubling model size provides consistent improvement (following a logarithmic pattern). 
                This motivated the growth from:
                <ul>
                    <li>GPT-2 (2019): 1.5 billion parameters</li>
                    <li>GPT-3 (2020): 175 billion parameters</li>
                    <li>GPT-4 (2023): estimated hundreds of billions</li>
                </ul>
                
                Each 10× increase in model size yields roughly constant improvement in performance—diminishing returns, but 
                returns nonetheless. This is a logarithmic relationship!
            </div>

            <h2>6.5 Logarithms in Data Preprocessing</h2>
            
            <p>
                Machine learning practitioners often apply log transforms to data:
            </p>

            <h3>6.5.1 Log Transform for Skewed Data</h3>
            <p>
                Many real-world datasets have skewed distributions (e.g., income, city populations, word frequencies). 
                Taking logarithms:
            </p>
            <ul>
                <li>Reduces skew, making distribution more symmetric</li>
                <li>Stabilizes variance across the range</li>
                <li>Makes relationships more linear</li>
                <li>Prevents very large values from dominating</li>
            </ul>

            <div class="example">
                <strong>Example: House Prices</strong><br>
                Raw prices might range from $100,000 to $10,000,000—a factor of 100. Taking log<sub>10</sub>:
                <ul>
                    <li>$100,000 → log<sub>10</sub>(100,000) = 5.0</li>
                    <li>$1,000,000 → log<sub>10</sub>(1,000,000) = 6.0</li>
                    <li>$10,000,000 → log<sub>10</sub>(10,000,000) = 7.0</li>
                </ul>
                
                Now the range is 5-7, much more manageable! Neural networks train better on this transformed data.
                
                When making predictions, you reverse the transform: if model predicts 6.2, the price is 10<sup>6.2</sup> ≈ $1,585,000.
            </div>

            <div class="highlight">
                <strong>Module Summary:</strong><br>
                Exponentials and logarithms are deeply embedded in modern AI:
                <ul>
                    <li><strong>Architecture:</strong> Sigmoid and softmax activation functions use exponentials</li>
                    <li><strong>Training:</strong> Cross-entropy loss uses logarithms; learning rate schedules use exponential decay</li>
                    <li><strong>Scaling:</strong> Model performance follows logarithmic/power-law relationships with size</li>
                    <li><strong>Data processing:</strong> Log transforms stabilize distributions and improve training</li>
                </ul>
                
                Understanding these mathematical foundations helps you understand <em>why</em> deep learning works, not just <em>that</em> it works.
            </div>
        </div>

        <div id="module7">
            <h1>Module 7: Additional Applications and Next Steps</h1>

            <h2>7.1 Signal Processing and Decibels</h2>
            
            <div class="historical">
                <strong>History of the Decibel:</strong>
                <ul>
                    <li><strong>1920s:</strong> Bell Telephone Laboratories needed a way to express power ratios in telephone circuits</li>
                    <li><strong>1924:</strong> "Bel" unit named after Alexander Graham Bell. Defined as log<sub>10</sub> of power ratio.</li>
                    <li><strong>Practical Problem:</strong> The bel was too large for convenient use</li>
                    <li><strong>Solution:</strong> Decibel (dB) = 1/10 of a bel became the standard unit</li>
                    <li><strong>Why Logarithmic?</strong> Human perception of loudness is approximately logarithmic (Weber-Fechner law). 
                    A logarithmic scale matches how we actually hear.</li>
                    <li><strong>1930s-present:</strong> Decibel scale extended beyond audio to radio, radar, electronics, anywhere 
                    power ratios span many orders of magnitude</li>
                </ul>
            </div>

            <p>
                Sound intensity spans from threshold of hearing (10<sup>-12</sup> W/m²) to jet engines (10² W/m²)—a 
                factor of 10<sup>14</sup>! The decibel scale compresses this enormous range:
            </p>
            
            <div class="formula">
                dB = 10 log<sub>10</sub>(I/I<sub>0</sub>)
            </div>
            <p style="text-align: center; font-size: 0.95em; color: #666;">
                where I<sub>0</sub> = 10<sup>-12</sup> W/m² (threshold of hearing, defined as 0 dB)
            </p>

            <table>
                <tr>
                    <th>Sound</th>
                    <th>Intensity (W/m²)</th>
                    <th>Decibels</th>
                </tr>
                <tr><td>Threshold of hearing</td><td>10<sup>-12</sup></td><td>0 dB</td></tr>
                <tr><td>Whisper</td><td>10<sup>-10</sup></td><td>20 dB</td></tr>
                <tr><td>Normal conversation</td><td>10<sup>-6</sup></td><td>60 dB</td></tr>
                <tr><td>Busy traffic</td><td>10<sup>-5</sup></td><td>70 dB</td></tr>
                <tr><td>Rock concert</td><td>10<sup>-1</sup></td><td>110 dB</td></tr>
                <tr><td>Jet engine</td><td>10²</td><td>140 dB (pain threshold)</td></tr>
            </table>

            <div class="example">
                <strong>Understanding Decibels:</strong>
                <ul>
                    <li>+10 dB = 10× more intensity (one order of magnitude)</li>
                    <li>+20 dB = 100× more intensity</li>
                    <li>+30 dB = 1,000× more intensity</li>
                    <li>Every 3 dB ≈ doubling of intensity (because log<sub>10</sub>(2) ≈ 0.3)</li>
                </ul>
                
                Normal conversation (60 dB) is 10<sup>6</sup> = 1,000,000 times more intense than threshold of hearing!
            </div>

            <h2>7.2 Compound Interest and Finance</h2>
            
            <div class="historical">
                <strong>History of Compound Interest:</strong>
                <ul>
                    <li><strong>Ancient times:</strong> Simple interest used (interest calculated only on principal)</li>
                    <li><strong>Medieval period:</strong> Compound interest developed in Italian banking centers (Florence, Venice)</li>
                    <li><strong>1683:</strong> Jacob Bernoulli studied compound interest mathematically, approaching the number e</li>
                    <li><strong>1697:</strong> Johann Bernoulli showed that continuous compounding limit is e<sup>rt</sup></li>
                    <li><strong>18th-19th century:</strong> Logarithm tables used to calculate compound interest and annuities</li>
                    <li><strong>Modern era:</strong> Exponential functions fundamental to all financial calculations: 
                    mortgages, bonds, retirement planning, options pricing</li>
                </ul>
            </div>

            <p>For continuous compounding (interest calculated and added continuously):</p>
            <div class="formula">
                A = Pe<sup>rt</sup>
            </div>
            <p style="text-align: center; font-size: 0.95em; color: #666;">
                where:<br>
                A = final amount<br>
                P = principal (initial investment)<br>
                r = annual interest rate (as decimal, so 7% = 0.07)<br>
                t = time in years<br>
                e ≈ 2.71828
            </p>

            <div class="example">
                <strong>Compound Interest Example:</strong><br>
                Invest $1,000 at 7% annual interest, continuously compounded:
                <ul>
                    <li>After 10 years: A = 1000 · e<sup>0.07×10</sup> = 1000 · e<sup>0.7</sup> ≈ 1000 · 2.014 = $2,014</li>
                    <li>After 20 years: A = 1000 · e<sup>1.4</sup> ≈ $4,055</li>
                    <li>After 30 years: A = 1000 · e<sup>2.1</sup> ≈ $8,166</li>
                </ul>
                
                Money roughly doubles every 10 years at 7% (this is the "Rule of 72" which we'll explain next).
            </div>

            <div class="graph-container">
                <div class="graph-title">Fig. 7.2-1: $1000 at 7% Annual Interest (Continuous Compounding)</div>
                <div class="plotly-wrapper" style="position: relative; max-width: 700px; margin: 0 auto;">
                    <div id="interestGraph" class="plotly-graph" style="width:100%; height:400px;"></div>
                </div>
                <div class="graph-caption">
                    Money doubles approximately every 10 years at 7% continuous compounding.
                </div>
            </div>

            <div class="highlight">
                <strong>Rule of 72:</strong> To estimate doubling time, divide 72 by the interest rate percentage. 
                <br><br>
                Examples:
                <ul>
                    <li>At 6% interest, money doubles in 72/6 = 12 years</li>
                    <li>At 8% interest, money doubles in 72/8 = 9 years</li>
                    <li>At 2% interest, money doubles in 72/2 = 36 years</li>
                </ul>
                
                <strong>Why does this work?</strong> For doubling, we need 2 = e<sup>rt</sup>, so rt = ln(2) ≈ 0.693. 
                Multiplying both sides by 100 gives (r as percentage) × t ≈ 69.3, rounded to 72 for divisibility by many numbers.
            </div>

            <h2>7.3 Cryptography: Keeping Secrets with Mathematics</h2>
            
            <div class="historical">
                <strong>The Quest for Secret Communication:</strong>
                <ul>
                    <li><strong>Ancient times:</strong> Julius Caesar used simple letter substitution ciphers (shift each letter by 3)</li>
                    <li><strong>Before 1976:</strong> All secret codes required both parties to share a secret key in advance. The key 
                    distribution problem: how do two people agree on a secret password if they can't meet in person and all communication 
                    might be intercepted?</li>
                    <li><strong>1976:</strong> Whitfield Diffie and Martin Hellman had a revolutionary idea: what if you could have a 
                    "public key" that anyone could see and use to encrypt messages, but only a secret "private key" could decrypt them?</li>
                    <li><strong>1977:</strong> Ron Rivest, Adi Shamir, and Leonard Adleman invented RSA—the first practical public-key 
                    encryption system. They realized that certain mathematical problems involving large numbers are easy in one direction 
                    but nearly impossible in reverse!</li>
                    <li><strong>Impact:</strong> Every time you see "https://" in your browser, every time you use online banking, 
                    every encrypted email—it's using RSA or similar public-key cryptography.</li>
                </ul>
            </div>

            <h3>Why This Belongs in a Lesson on Exponentials and Logarithms</h3>
            
            <p>
                RSA encryption works because of a mathematical asymmetry involving exponents:
            </p>
            
            <div class="example">
                <strong>The Basic Idea (Simplified):</strong>
                
                <p><strong>Easy Direction:</strong> Computing large powers is easy<em>*</em><br>
                Want to compute 7<sup>82</sup>? Even for huge exponents (hundreds of digits), computers can do this quickly using a clever 
                trick involving logarithms.</p>
                
                <p><strong>Hard Direction:</strong> Working backwards is nearly impossible<br>
                If I tell you the answer is 49, can you figure out what power of 7 gives 49? For small numbers like this, yes. But if I 
                give you a 300-digit number and ask "what power of 7 gives this result?", there's no known efficient method! This is called 
                the <strong>discrete logarithm problem</strong>—it's like trying to find a logarithm, but in a special mathematical system.</p>
                
                <p style="font-size: 0.9em; margin-top: 15px;">
                <em>*There's a catch: in RSA, we work with "modular arithmetic" (clock arithmetic—like how 15 mod 12 = 3 on a 12-hour clock). 
                The full version is: computing b<sup>e</sup> mod n is easy, but finding e from the result is hard. The "mod n" part is what 
                makes it secure.</em>
                </p>
            </div>

            <h3>The Logarithmic Trick: Fast Exponentiation</h3>
            
            <p>
                Here's where logarithms make encryption practical. To compute something like 7<sup>82</sup>, you might think you need to 
                multiply 7 by itself 82 times—81 multiplication operations. For the huge numbers in cryptography (exponents with hundreds 
                of digits), that would take longer than the age of the universe!
            </p>
            
            <div class="example">
                <strong>The Fast Method (Binary Exponentiation):</strong><br><br>
                
                <strong>Step 1:</strong> Write the exponent in binary (base 2). <br>
                82 in binary is 1010010, which means 82 = 64 + 16 + 2<br><br>
                
                <strong>Step 2:</strong> Build up powers by repeatedly squaring:<br>
                7<sup>1</sup> = 7<br>
                7<sup>2</sup> = 7 × 7 = 49<br>
                7<sup>4</sup> = 49 × 49 = 2,401<br>
                7<sup>8</sup> = 2,401 × 2,401 = 5,764,801<br>
                7<sup>16</sup> = 5,764,801 × 5,764,801 (and so on...)<br>
                7<sup>64</sup> = keep squaring...<br><br>
                
                <strong>Step 3:</strong> Combine the right powers:<br>
                7<sup>82</sup> = 7<sup>64</sup> × 7<sup>16</sup> × 7<sup>2</sup><br><br>
                
                <strong>How many operations?</strong> Instead of 81 multiplications, we only need about log<sub>2</sub>(82) ≈ 7 squaring 
                operations! For a 300-digit exponent, we'd need only about log<sub>2</sub>(10<sup>300</sup>) ≈ 1000 operations instead of 
                10<sup>300</sup> operations (more atoms than in the universe!).
            </div>

            <div class="highlight">
                <strong>Why This Matters for Computing:</strong><br>
                
                This shows the power of understanding logarithmic scaling:
                <ul>
                    <li><strong>Secure internet commerce is possible</strong> because we can compute huge exponentials quickly 
                    (in logarithmic time)</li>
                    <li><strong>Our data stays secure</strong> because the reverse operation (discrete logarithm) takes exponential time—
                    even the fastest supercomputers would need billions of years</li>
                    <li><strong>The mathematical gap</strong> between these two—one problem taking log(n) steps, the reverse taking 2<sup>n</sup> 
                    steps—is what makes modern cryptography work</li>
                </ul>
                
                When you buy something online, your credit card number is encrypted using exactly this principle. The merchant's website 
                has a public key (anyone can see it). Your browser uses that public key and fast exponentiation to encrypt your card number. 
                Only the merchant has the private key needed to decrypt it. An eavesdropper seeing the encrypted data would need to solve 
                the discrete logarithm problem—effectively impossible with current technology.
            </div>

            <div class="warning">
                <strong>Looking Forward:</strong> Quantum computers, if they become practical, could break RSA by solving discrete 
                logarithms efficiently. This has sparked research in "post-quantum cryptography"—new encryption methods that even 
                quantum computers can't break. The mathematical arms race continues!
            </div>

            <h2>7.4 Historical Context: Power Laws</h2>
            
            <div class="historical">
                <strong>Discovery of Power Laws in Nature:</strong>
                <ul>
                    <li><strong>1609:</strong> Kepler's laws of planetary motion included power relationships (T² ∝ a³)</li>
                    <li><strong>1686:</strong> Newton's inverse square law for gravity: F ∝ 1/r²</li>
                    <li><strong>1785:</strong> Coulomb's inverse square law for electrical force</li>
                    <li><strong>1879-1884:</strong> Stefan-Boltzmann law: blackbody radiation E ∝ T<sup>4</sup></li>
                    <li><strong>1906:</strong> Pareto distribution (80/20 rule)—power law in economics</li>
                    <li><strong>1932:</strong> Kleiber's law: metabolic rate ∝ M<sup>3/4</sup></li>
                    <li><strong>1950s-present:</strong> Power laws found throughout physics, biology, economics, internet, social networks</li>
                </ul>
                
                <strong>Why Logarithms Reveal Power Laws:</strong> If y = kx<sup>α</sup> (power law), then log(y) = log(k) + α log(x)—
                a linear relationship! On a log-log plot, power laws appear as straight lines with slope α. This makes them easy 
                to identify and measure.
            </div>

            <p>
                Power laws—relationships where one quantity varies as a power of another—appear throughout nature. 
                Logarithms reveal these patterns: on log-log scales, power laws appear as straight lines.
            </p>

            <div class="example">
                <strong>Famous Power Laws:</strong>
                <ul>
                    <li><strong>Inverse Square Laws</strong> (Newton 1686, Coulomb 1785): Gravitational and electrical 
                    forces F ∝ 1/r². Doubling distance reduces force to 1/4. These laws emerge from the geometry of 
                    three-dimensional space—force spreads over a sphere of area 4πr².</li>
                    
                    <li><strong>Stefan-Boltzmann Law</strong> (Stefan 1879, Boltzmann 1884): Blackbody radiation power 
                    P = σAT<sup>4</sup>. Doubling temperature increases radiation by 2<sup>4</sup> = 16×. Explains why 
                    hot objects glow—even small temperature increases cause dramatic radiation increases.</li>
                    
                    <li><strong>Kleiber's Law</strong> (Kleiber 1932): Metabolic rate BMR ∝ M<sup>3/4</sup> where M is body mass. 
                    A cat 100× heavier than a mouse consumes only 100<sup>3/4</sup> ≈ 32× the energy—smaller animals have 
                    proportionally higher metabolic rates. This explains why shrews must eat constantly while elephants can go days 
                    between meals.</li>
                    
                    <li><strong>Zipf's Law</strong> (George Zipf 1935): In language, the nth most common word appears with 
                    frequency ∝ 1/n. In English, "the" appears ~7% of time, "of" ~3.5%, "and" ~2.3%, etc. Similar power laws appear 
                    in city sizes, website popularity, wealth distribution—anywhere with "rich get richer" dynamics.</li>
                </ul>
            </div>

            <h2>7.5 Summary and Next Steps</h2>
            <p>
                Over these seven modules, you've seen exponentials and logarithms across history, chemistry, biology, physics, 
                astronomy, medicine, computing, AI/ML, and engineering. These mathematical tools are not just abstractions—they're 
                essential for understanding the patterns that govern our world.
            </p>

            <div class="highlight">
                <strong>Why This Matters for Your Future</strong><br>
                As someone becoming a computer expert, exponentials and logarithms will appear constantly:
                <ul>
                    <li><strong>Algorithm analysis:</strong> Understanding O(log n) vs O(n) vs O(n²) complexity</li>
                    <li><strong>Data structures:</strong> Binary trees, heaps, hash tables—all involve logarithms</li>
                    <li><strong>Machine learning:</strong> Activation functions, loss functions, scaling laws</li>
                    <li><strong>Cryptography:</strong> Modular exponentiation, discrete logarithms</li>
                    <li><strong>Systems design:</strong> Understanding exponential growth, capacity planning</li>
                    <li><strong>Compression:</strong> Entropy, Huffman coding, information theory</li>
                    <li><strong>Networking:</strong> Distributed hash tables, routing protocols</li>
                </ul>
                
                More fundamentally: exponentials and logarithms are the language of <em>scale</em>. Whether you're optimizing code, 
                designing systems, or analyzing data, you need to reason about how things scale from small to large. These mathematical 
                tools let you think precisely about scaling—not just "this is fast" or "this is slow," but quantitatively 
                understanding trade-offs.
            </div>

            <div class="warning">
                <strong>Recommended Next Steps:</strong>
                <ol>
                    <li><strong>Calculus:</strong> Shows <em>why</em> exponential patterns emerge from "rate proportional to amount." 
                    Derivatives of e<sup>x</sup> and properties of ln(x) become clearer. Understanding limits, series, and differential 
                    equations reveals the deep connections.</li>
                    
                    <li><strong>Discrete Mathematics:</strong> Formal treatment of logarithms in algorithm analysis. Recurrence relations, 
                    master theorem for divide-and-conquer algorithms, generating functions. Essential for theoretical computer science.</li>
                    
                    <li><strong>Information Theory:</strong> Shannon's theory connecting logarithms, entropy, and communication. 
                    Fundamental for understanding compression, error correction, cryptography, and even machine learning.</li>
                    
                    <li><strong>Probability & Statistics:</strong> Log-normal distributions, maximum likelihood estimation, exponential 
                    families. Understanding why logarithms appear in statistical inference.</li>
                    
                    <li><strong>Linear Algebra:</strong> Matrix exponentials, eigenvalue analysis, graph algorithms. Exponentials of 
                    matrices define dynamical systems and Markov chains.</li>
                    
                    <li><strong>Practical Coding:</strong> Implement binary search, balanced trees, sorting algorithms. Use profiling 
                    tools to measure real algorithm complexity. Build a small search engine or database index.</li>
                </ol>
            </div>

            <div class="highlight" style="background-color: #d1fae5; border-left-color: #10b981;">
                <strong>Final Thoughts</strong><br>
                John Napier created logarithms in 1614 to save astronomers from tedious calculations. For 350 years, logarithms 
                were primarily computational tools—ways to turn multiplication into addition, ways to compress large numbers into 
                manageable ranges.<br><br>
                
                Then computers arrived in the mid-20th century and eliminated that use almost overnight. Electronic calculators 
                could multiply large numbers instantly. Log tables became obsolete. The slide rule, icon of engineering for 70 years, 
                vanished from campuses by 1980.<br><br>
                
                Yet logarithms didn't become less important—they became <em>more</em> important! As computing advanced, we discovered 
                that logarithms are fundamental to:
                <ul>
                    <li>Understanding algorithm efficiency (why some programs scale beautifully while others choke on large data)</li>
                    <li>Designing data structures (trees, indexes, hash tables)</li>
                    <li>Measuring information (bits, entropy, compression limits)</li>
                    <li>Training neural networks (activation functions, loss functions, learning rates)</li>
                    <li>Securing communications (cryptography, discrete logarithm problem)</li>
                    <li>Scaling systems (understanding exponential growth, capacity planning)</li>
                </ul>
                
                The mathematical patterns Napier discovered while trying to help astronomers turned out to be fundamental to the 
                digital age. That's the beauty of mathematics: solve one problem, and you've given humanity a tool that finds 
                applications centuries later in ways the original inventor never imagined.<br><br>
                
                As you continue your journey into computer science, you'll encounter exponentials and logarithms again and again. 
                Each time, take a moment to appreciate that you're using tools with four centuries of history, tools that connect 
                ancient problems to cutting-edge technology. Mathematics is not just a subject you study—it's a language for 
                understanding patterns in the universe, patterns that persist across time and across disciplines.
            </div>
        </div>

        <hr style="margin: 50px 0; border: none; border-top: 2px solid #e5e7eb;">
        <p style="text-align: center; color: #6b7280; font-size: 0.9em;">
            © 2025 Exponentials and Logarithms: A Comprehensive Lesson<br>
            For educational use. All graphs are SVG embedded in this document.
        </p>
    </div>
    
    

    <!-- Plotly.js Graph Initialization -->
    <script>
    document.addEventListener('DOMContentLoaded', function() {
        
        // Custom fullscreen button for Plotly modebar
        const fullscreenButton = {
            name: 'Fullscreen',
            icon: {
                width: 24,
                height: 24,
                path: 'M4 4h6v2H6v4H4V4zm16 0v6h-2V6h-4V4h6zM4 20v-6h2v4h4v2H4zm16 0h-6v-2h4v-4h2v6z'
            },
            click: function(gd) {
                const container = gd.closest('.graph-container');
                if (document.fullscreenElement) {
                    document.exitFullscreen();
                } else {
                    container.requestFullscreen();
                }
            }
        };

        // Standard layout settings
        const standardLayout = {
            plot_bgcolor: 'white',
            paper_bgcolor: '#f9fafb',
            margin: { l: 60, r: 30, t: 30, b: 50 },
            showlegend: true,
            legend: { x: 0.02, y: 0.98 }
        };
        
        const standardConfig = {
            responsive: true,
            displayModeBar: true,
            modeBarButtonsToAdd: [fullscreenButton],
            modeBarButtonsToRemove: [
                'select2d', 'lasso2d', 'autoScale2d',
                'hoverClosestCartesian', 'hoverCompareCartesian', 
                'toggleSpikelines'
            ],
            displaylogo: false
        };
        
        // ==================== GRAPH 1: Logarithms in Different Bases ====================
        (function() {
            const x = [], log2 = [], ln = [], log10 = [];
            for (let i = 0.1; i <= 1000; i += 5) {
                x.push(i);
                log2.push(Math.log2(i));
                ln.push(Math.log(i));
                log10.push(Math.log10(i));
            }
            
            const traces = [
                { x: x, y: log2, type: 'scatter', mode: 'lines', name: 'log₂(x)', line: { color: '#3b82f6', width: 2 } },
                { x: x, y: ln, type: 'scatter', mode: 'lines', name: 'ln(x)', line: { color: '#f59e0b', width: 2 } },
                { x: x, y: log10, type: 'scatter', mode: 'lines', name: 'log₁₀(x)', line: { color: '#10b981', width: 2 } },
                { x: [1], y: [0], type: 'scatter', mode: 'markers', name: 'Point (1,0)', marker: { color: '#dc2626', size: 10 } }
            ];
            
            const layout = {
                ...standardLayout,
                xaxis: { title: 'x', range: [0, 1000], gridcolor: '#e5e7eb' },
                yaxis: { title: 'log(x)', range: [-1, 10], gridcolor: '#e5e7eb' }
            };
            
            Plotly.newPlot('logBasesGraph', traces, layout, standardConfig);
        })();
        
        // ==================== GRAPH 2: Exponential Function y = 2^x ====================
        (function() {
            const x = [], y = [];
            for (let i = -3; i <= 4; i += 0.1) {
                x.push(i);
                y.push(Math.pow(2, i));
            }
            
            const traces = [
                { x: x, y: y, type: 'scatter', mode: 'lines', name: 'y = 2^x', line: { color: '#3b82f6', width: 2 } },
                { x: [0], y: [1], type: 'scatter', mode: 'markers', name: '(0, 1)', marker: { color: '#dc2626', size: 10 } }
            ];
            
            const layout = {
                ...standardLayout,
                xaxis: { title: 'x', range: [-3, 4], gridcolor: '#e5e7eb' },
                yaxis: { title: 'y', range: [0, 16], gridcolor: '#e5e7eb' }
            };
            
            Plotly.newPlot('exp2xGraph', traces, layout, standardConfig);
        })();
        
        // ==================== GRAPH 3: Exponential Decay y = e^(-x) ====================
        (function() {
            const x = [], y = [];
            for (let i = -1; i <= 5; i += 0.1) {
                x.push(i);
                y.push(Math.exp(-i));
            }
            
            const traces = [
                { x: x, y: y, type: 'scatter', mode: 'lines', name: 'y = e^(-x)', line: { color: '#ef4444', width: 2 } },
                { x: [0], y: [1], type: 'scatter', mode: 'markers', name: '(0, 1)', marker: { color: '#dc2626', size: 10 } }
            ];
            
            const layout = {
                ...standardLayout,
                xaxis: { title: 'x', range: [-1, 5], gridcolor: '#e5e7eb' },
                yaxis: { title: 'y', range: [0, 3], gridcolor: '#e5e7eb' }
            };
            
            Plotly.newPlot('expNegXGraph', traces, layout, standardConfig);
        })();
        
        // ==================== GRAPH 4: Logarithmic Function y = ln(x) ====================
        (function() {
            const x = [], y = [];
            for (let i = 0.01; i <= 10; i += 0.1) {
                x.push(i);
                y.push(Math.log(i));
            }
            
            const traces = [
                { x: x, y: y, type: 'scatter', mode: 'lines', name: 'y = ln(x)', line: { color: '#10b981', width: 2 } },
                { x: [1], y: [0], type: 'scatter', mode: 'markers', name: '(1, 0)', marker: { color: '#dc2626', size: 10 } }
            ];
            
            const layout = {
                ...standardLayout,
                xaxis: { title: 'x', range: [0, 10], gridcolor: '#e5e7eb' },
                yaxis: { title: 'y', range: [-5, 3], gridcolor: '#e5e7eb' }
            };
            
            Plotly.newPlot('lnGraph', traces, layout, standardConfig);
        })();
        
        // ==================== GRAPH 5: Inverse Functions ====================
        (function() {
            const xExp = [], yExp = [], xLn = [], yLn = [], xLine = [], yLine = [];
            
            for (let i = -2; i <= 2.5; i += 0.05) {
                xExp.push(i);
                yExp.push(Math.exp(i));
            }
            for (let i = 0.05; i <= 10; i += 0.1) {
                xLn.push(i);
                yLn.push(Math.log(i));
            }
            for (let i = -2; i <= 10; i += 0.5) {
                xLine.push(i);
                yLine.push(i);
            }
            
            const traces = [
                { x: xExp, y: yExp, type: 'scatter', mode: 'lines', name: 'y = e^x', line: { color: '#3b82f6', width: 2 } },
                { x: xLn, y: yLn, type: 'scatter', mode: 'lines', name: 'y = ln(x)', line: { color: '#10b981', width: 2 } },
                { x: xLine, y: yLine, type: 'scatter', mode: 'lines', name: 'y = x', line: { color: '#9ca3af', width: 2, dash: 'dash' } }
            ];
            
            const layout = {
                ...standardLayout,
                xaxis: { title: 'x', range: [-2, 10], gridcolor: '#e5e7eb', zeroline: true, zerolinecolor: '#9ca3af' },
                yaxis: { title: 'y', range: [-2, 10], gridcolor: '#e5e7eb', zeroline: true, zerolinecolor: '#9ca3af' }
            };
            
            Plotly.newPlot('inverseGraph', traces, layout, standardConfig);
        })();
        
        // ==================== GRAPH 6: Linear-Linear Plot ====================
        (function() {
            const x = [], y = [];
            for (let i = 0; i <= 10; i += 0.1) {
                x.push(i);
                y.push(Math.exp(0.5 * i));
            }
            
            const traces = [
                { x: x, y: y, type: 'scatter', mode: 'lines', name: 'y = e^(0.5x)', line: { color: '#3b82f6', width: 2 } }
            ];
            
            const layout = {
                ...standardLayout,
                xaxis: { title: 'x (linear scale)', range: [0, 10], gridcolor: '#e5e7eb' },
                yaxis: { title: 'y (linear scale)', range: [0, 160], gridcolor: '#e5e7eb' }
            };
            
            Plotly.newPlot('linearLinearGraph', traces, layout, standardConfig);
        })();
        
        // ==================== GRAPH 7: Semi-Log Plot (LOG Y-AXIS) ====================
        (function() {
            const x = [], y = [];
            for (let i = 0; i <= 10; i += 0.1) {
                x.push(i);
                y.push(Math.exp(0.5 * i));
            }
            
            const traces = [
                { x: x, y: y, type: 'scatter', mode: 'lines', name: 'y = e^(0.5x)', line: { color: '#3b82f6', width: 2 } }
            ];
            
            const layout = {
                ...standardLayout,
                xaxis: { title: 'x (linear scale)', range: [0, 10], gridcolor: '#e5e7eb' },
                yaxis: { 
                    title: 'y (logarithmic scale)', 
                    type: 'log',
                    range: [-1, 3],  // log scale: 10^-1 to 10^3
                    gridcolor: '#e5e7eb',
                    dtick: 1
                }
            };
            
            Plotly.newPlot('semiLogGraph', traces, layout, standardConfig);
        })();
        
        // ==================== GRAPH 8: Log-Log Plot (BOTH AXES LOG) ====================
        (function() {
            const x = [], y1 = [], y2 = [], y3 = [];
            for (let i = 0.1; i <= 100; i *= 1.1) {
                x.push(i);
                y1.push(i * i);        // y = x^2
                y2.push(i);            // y = x
                y3.push(Math.sqrt(i)); // y = sqrt(x)
            }
            
            const traces = [
                { x: x, y: y1, type: 'scatter', mode: 'lines', name: 'y = x² (slope 2)', line: { color: '#3b82f6', width: 2 } },
                { x: x, y: y2, type: 'scatter', mode: 'lines', name: 'y = x (slope 1)', line: { color: '#10b981', width: 2 } },
                { x: x, y: y3, type: 'scatter', mode: 'lines', name: 'y = √x (slope 0.5)', line: { color: '#f59e0b', width: 2 } }
            ];
            
            const layout = {
                ...standardLayout,
                xaxis: { 
                    title: 'x (logarithmic scale)', 
                    type: 'log',
                    range: [-1, 2],  // 10^-1 to 10^2
                    gridcolor: '#e5e7eb',
                    dtick: 1
                },
                yaxis: { 
                    title: 'y (logarithmic scale)', 
                    type: 'log',
                    range: [-1, 4],  // 10^-1 to 10^4
                    gridcolor: '#e5e7eb',
                    dtick: 1
                }
            };
            
            Plotly.newPlot('logLogGraph', traces, layout, standardConfig);
        })();
        
        // ==================== GRAPH 9a: Bacterial Growth - Linear (Early Phase) ====================
        (function() {
            const t = [], expGrowth = [], logisticGrowth = [];
            const K = 1000;  // Carrying capacity
            const r = 0.5;   // Growth rate
            const N0 = 10;   // Initial population
            
            // Early phase only: 0 to 8 hours
            for (let i = 0; i <= 8; i += 0.2) {
                t.push(i);
                expGrowth.push(N0 * Math.exp(r * i));
                logisticGrowth.push(K / (1 + ((K - N0)/N0) * Math.exp(-r * i)));
            }
            
            const traces = [
                { x: t, y: expGrowth, type: 'scatter', mode: 'lines', name: 'Exponential: N₀·e^(rt)', line: { color: '#3b82f6', width: 2 } },
                { x: t, y: logisticGrowth, type: 'scatter', mode: 'lines', name: 'Logistic: K/(1+...)', line: { color: '#10b981', width: 2 } },
                { x: [0, 8], y: [K, K], type: 'scatter', mode: 'lines', name: 'Carrying Capacity K', line: { color: '#9ca3af', width: 2, dash: 'dash' } }
            ];
            
            const layout = {
                ...standardLayout,
                xaxis: { title: 'Time (hours)', range: [0, 8], gridcolor: '#e5e7eb' },
                yaxis: { title: 'Population', range: [0, 1500], gridcolor: '#e5e7eb' }
            };
            
            Plotly.newPlot('bacterialGraphLinear', traces, layout, standardConfig);
        })();
        
        // ==================== GRAPH 9b: Bacterial Growth - Semi-Log (Full Range) ====================
        (function() {
            const t = [], expGrowth = [], logisticGrowth = [];
            const K = 1000;  // Carrying capacity
            const r = 0.5;   // Growth rate
            const N0 = 10;   // Initial population
            
            // Full range: 0 to 24 hours
            for (let i = 0; i <= 24; i += 0.5) {
                t.push(i);
                expGrowth.push(N0 * Math.exp(r * i));
                logisticGrowth.push(K / (1 + ((K - N0)/N0) * Math.exp(-r * i)));
            }
            
            const traces = [
                { x: t, y: expGrowth, type: 'scatter', mode: 'lines', name: 'Exponential: N₀·e^(rt)', line: { color: '#3b82f6', width: 2 } },
                { x: t, y: logisticGrowth, type: 'scatter', mode: 'lines', name: 'Logistic: K/(1+...)', line: { color: '#10b981', width: 2 } },
                { x: [0, 24], y: [K, K], type: 'scatter', mode: 'lines', name: 'Carrying Capacity K', line: { color: '#9ca3af', width: 2, dash: 'dash' } }
            ];
            
            const layout = {
                ...standardLayout,
                xaxis: { title: 'Time (hours)', range: [0, 24], gridcolor: '#e5e7eb' },
                yaxis: { 
                    title: 'Population (log scale)', 
                    type: 'log',
                    range: [0.5, 5.5],  // ~3 to ~300,000
                    gridcolor: '#e5e7eb',
                    dtick: 1
                }
            };
            
            Plotly.newPlot('bacterialGraphLog', traces, layout, standardConfig);
        })();
        
        // ==================== GRAPH 10: Carbon-14 Decay ====================
        (function() {
            const t = [], remaining = [];
            const halfLife = 5730;  // years
            const k = Math.log(2) / halfLife;
            
            for (let i = 0; i <= 30000; i += 500) {
                t.push(i);
                remaining.push(100 * Math.exp(-k * i));
            }
            
            // Half-life markers
            const halfLifeTimes = [5730, 11460, 17190, 22920];
            const halfLifePercents = halfLifeTimes.map(time => 100 * Math.exp(-k * time));
            
            const traces = [
                { x: t, y: remaining, type: 'scatter', mode: 'lines', name: 'C-14 Remaining (%)', line: { color: '#3b82f6', width: 2 } },
                { x: halfLifeTimes, y: halfLifePercents, type: 'scatter', mode: 'markers', name: 'Half-life points', marker: { color: '#dc2626', size: 8 } }
            ];
            
            const layout = {
                ...standardLayout,
                xaxis: { title: 'Time (years)', range: [0, 30000], gridcolor: '#e5e7eb' },
                yaxis: { title: 'Remaining C-14 (%)', range: [0, 100], gridcolor: '#e5e7eb' },
                annotations: [
                    { x: 5730, y: 50, text: '50% at 5,730 years', showarrow: true, arrowhead: 2, ax: 40, ay: -30 }
                ]
            };
            
            Plotly.newPlot('carbon14Graph', traces, layout, standardConfig);
        })();
        
        // ==================== GRAPH 11: Coffee Cooling ====================
        (function() {
            const t = [], temp = [];
            const T0 = 90;      // Initial temp
            const Tenv = 20;    // Room temp
            const k = 0.1;      // Cooling constant
            
            for (let i = 0; i <= 60; i += 1) {
                t.push(i);
                // Newton's Law of Cooling: T(t) = Tenv + (T0 - Tenv) * e^(-kt)
                temp.push(Tenv + (T0 - Tenv) * Math.exp(-k * i));
            }
            
            const traces = [
                { x: t, y: temp, type: 'scatter', mode: 'lines', name: 'Coffee Temperature', line: { color: '#f59e0b', width: 2 } },
                { x: [0, 60], y: [Tenv, Tenv], type: 'scatter', mode: 'lines', name: 'Room Temperature', line: { color: '#9ca3af', width: 2, dash: 'dash' } }
            ];
            
            const layout = {
                ...standardLayout,
                xaxis: { title: 'Time (minutes)', range: [0, 60], gridcolor: '#e5e7eb' },
                yaxis: { title: 'Temperature (°C)', range: [15, 95], gridcolor: '#e5e7eb' }
            };
            
            Plotly.newPlot('coffeeCoolingGraph', traces, layout, standardConfig);
        })();
        
        // ==================== GRAPH 12a: Algorithm Complexity - Linear (Small n) ====================
        (function() {
            const n = [], constant = [], logN = [], linear = [], nLogN = [], quadratic = [];
            
            // Small n: 1 to 20
            for (let i = 1; i <= 20; i += 1) {
                n.push(i);
                constant.push(1);
                logN.push(Math.log2(i));
                linear.push(i);
                nLogN.push(i * Math.log2(i));
                quadratic.push(i * i);
            }
            
            const traces = [
                { x: n, y: constant, type: 'scatter', mode: 'lines', name: 'O(1)', line: { color: '#10b981', width: 2 } },
                { x: n, y: logN, type: 'scatter', mode: 'lines', name: 'O(log n)', line: { color: '#3b82f6', width: 2 } },
                { x: n, y: linear, type: 'scatter', mode: 'lines', name: 'O(n)', line: { color: '#f59e0b', width: 2 } },
                { x: n, y: nLogN, type: 'scatter', mode: 'lines', name: 'O(n log n)', line: { color: '#8b5cf6', width: 2 } },
                { x: n, y: quadratic, type: 'scatter', mode: 'lines', name: 'O(n²)', line: { color: '#ef4444', width: 2 } }
            ];
            
            const layout = {
                ...standardLayout,
                xaxis: { title: 'Input size (n)', range: [0, 20], gridcolor: '#e5e7eb' },
                yaxis: { title: 'Operations', range: [0, 450], gridcolor: '#e5e7eb' }
            };
            
            Plotly.newPlot('algorithmGraphLinear', traces, layout, standardConfig);
        })();
        
        // ==================== GRAPH 12b: Algorithm Complexity - Semi-Log (Large n) ====================
        (function() {
            const n = [], constant = [], logN = [], linear = [], nLogN = [], quadratic = [];
            
            // Full range: 1 to 100
            for (let i = 1; i <= 100; i += 1) {
                n.push(i);
                constant.push(1);
                logN.push(Math.log2(i));
                linear.push(i);
                nLogN.push(i * Math.log2(i));
                quadratic.push(i * i);
            }
            
            const traces = [
                { x: n, y: constant, type: 'scatter', mode: 'lines', name: 'O(1)', line: { color: '#10b981', width: 2 } },
                { x: n, y: logN, type: 'scatter', mode: 'lines', name: 'O(log n)', line: { color: '#3b82f6', width: 2 } },
                { x: n, y: linear, type: 'scatter', mode: 'lines', name: 'O(n)', line: { color: '#f59e0b', width: 2 } },
                { x: n, y: nLogN, type: 'scatter', mode: 'lines', name: 'O(n log n)', line: { color: '#8b5cf6', width: 2 } },
                { x: n, y: quadratic, type: 'scatter', mode: 'lines', name: 'O(n²)', line: { color: '#ef4444', width: 2 } }
            ];
            
            const layout = {
                ...standardLayout,
                xaxis: { title: 'Input size (n)', range: [0, 100], gridcolor: '#e5e7eb' },
                yaxis: { 
                    title: 'Operations (log scale)', 
                    type: 'log',
                    range: [-0.5, 4.5],  // ~0.3 to ~30,000
                    gridcolor: '#e5e7eb',
                    dtick: 1
                }
            };
            
            Plotly.newPlot('algorithmGraphLog', traces, layout, standardConfig);
        })();
        
        // ==================== GRAPH 13: Sigmoid Function ====================
        (function() {
            const x = [], y = [];
            for (let i = -6; i <= 6; i += 0.1) {
                x.push(i);
                y.push(1 / (1 + Math.exp(-i)));
            }
            
            const traces = [
                { x: x, y: y, type: 'scatter', mode: 'lines', name: 'σ(x) = 1/(1 + e^(-x))', line: { color: '#3b82f6', width: 2 } },
                { x: [0], y: [0.5], type: 'scatter', mode: 'markers', name: '(0, 0.5)', marker: { color: '#dc2626', size: 10 } }
            ];
            
            const layout = {
                ...standardLayout,
                xaxis: { title: 'x', range: [-6, 6], gridcolor: '#e5e7eb', zeroline: true, zerolinecolor: '#9ca3af' },
                yaxis: { title: 'σ(x)', range: [0, 1], gridcolor: '#e5e7eb' }
            };
            
            Plotly.newPlot('sigmoidGraph', traces, layout, standardConfig);
        })();
        
        // ==================== GRAPH 14: Softmax Function ====================
        (function() {
            const x = [], p1 = [], p2 = [], p3 = [];
            // Fixed: z2 = 1.0, z3 = 0.5
            const z2 = 1.0, z3 = 0.5;
            
            for (let z1 = -2; z1 <= 4; z1 += 0.1) {
                x.push(z1);
                const expSum = Math.exp(z1) + Math.exp(z2) + Math.exp(z3);
                p1.push(Math.exp(z1) / expSum);
                p2.push(Math.exp(z2) / expSum);
                p3.push(Math.exp(z3) / expSum);
            }
            
            const traces = [
                { x: x, y: p1, type: 'scatter', mode: 'lines', name: 'P(class 1)', line: { color: '#3b82f6', width: 2 } },
                { x: x, y: p2, type: 'scatter', mode: 'lines', name: 'P(class 2)', line: { color: '#10b981', width: 2 } },
                { x: x, y: p3, type: 'scatter', mode: 'lines', name: 'P(class 3)', line: { color: '#f59e0b', width: 2 } }
            ];
            
            const layout = {
                ...standardLayout,
                xaxis: { title: 'Input value x (other inputs: 1.0, 0.5)', range: [-2, 4], gridcolor: '#e5e7eb' },
                yaxis: { title: 'Probability', range: [0, 1], gridcolor: '#e5e7eb' }
            };
            
            Plotly.newPlot('softmaxGraph', traces, layout, standardConfig);
        })();
        
        // ==================== GRAPH 15: Cross-Entropy Loss ====================
        (function() {
            const p = [], loss = [];
            // For binary classification with true label = 1
            // Loss = -log(p) where p is predicted probability
            for (let i = 0.01; i <= 0.99; i += 0.01) {
                p.push(i);
                loss.push(-Math.log(i));
            }
            
            const traces = [
                { x: p, y: loss, type: 'scatter', mode: 'lines', name: 'Loss = -ln(p)', line: { color: '#dc2626', width: 2 } },
                { x: [0.1, 0.5, 0.9], y: [-Math.log(0.1), -Math.log(0.5), -Math.log(0.9)], 
                  type: 'scatter', mode: 'markers', name: 'Key points', marker: { color: '#3b82f6', size: 8 } }
            ];
            
            const layout = {
                ...standardLayout,
                xaxis: { title: 'Predicted Probability (p)', range: [0, 1], gridcolor: '#e5e7eb' },
                yaxis: { title: 'Loss', range: [0, 5], gridcolor: '#e5e7eb' }
            };
            
            Plotly.newPlot('crossEntropyGraph', traces, layout, standardConfig);
        })();
        
        // ==================== GRAPH 16: Compound Interest ====================
        (function() {
            const t = [], balance = [];
            const P = 1000;   // Principal
            const r = 0.07;   // 7% annual rate
            
            for (let i = 0; i <= 30; i += 0.5) {
                t.push(i);
                // Continuous compounding: A = P * e^(rt)
                balance.push(P * Math.exp(r * i));
            }
            
            // Key markers: doubling time at ~10 years, tripling at ~16 years
            const keyTimes = [0, 10, 20, 30];
            const keyBalances = keyTimes.map(time => P * Math.exp(r * time));
            
            const traces = [
                { x: t, y: balance, type: 'scatter', mode: 'lines', name: 'Balance', line: { color: '#10b981', width: 2 } },
                { x: keyTimes, y: keyBalances, type: 'scatter', mode: 'markers', name: 'Milestones', marker: { color: '#3b82f6', size: 8 } }
            ];
            
            const layout = {
                ...standardLayout,
                xaxis: { title: 'Years', range: [0, 30], gridcolor: '#e5e7eb' },
                yaxis: { title: 'Balance ($)', range: [0, 9000], gridcolor: '#e5e7eb' },
                annotations: [
                    { x: 10, y: P * Math.exp(r * 10), text: '~$2,014 at 10 years', showarrow: true, arrowhead: 2, ax: 40, ay: -30 }
                ]
            };
            
            Plotly.newPlot('interestGraph', traces, layout, standardConfig);
        })();
        
        // Fullscreen change handler - triggers Plotly resize
        // CSS handles sizing via .graph-container:fullscreen rules
        document.addEventListener('fullscreenchange', function() {
            const plotlyDivs = document.querySelectorAll('.plotly-graph');
            
            // Delay resize to let CSS take effect
            setTimeout(function() {
                plotlyDivs.forEach(function(graphDiv) {
                    Plotly.Plots.resize(graphDiv);
                });
            }, 200);
        });
        
    });
    </script>

</body>
</html>
